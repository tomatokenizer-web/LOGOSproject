# 핵심 언어 분석 모듈

> **번역 원본**: g2p.md, g2p-irt.md, morphology.md
> **번역 일자**: 2026-01-07

---

## 자소-음소 변환 (Grapheme-to-Phoneme, G2P) 분석 모듈

> **최종 업데이트**: 2026-01-04
> **코드 위치**: `src/core/g2p.ts`
> **상태**: 활성

---

### 맥락과 목적

이 모듈은 언어 학습의 근본적인 도전 과제를 해결하기 위해 존재합니다: 영어 철자법은 발음과의 일관성이 매우 낮습니다. "through", "though", "tough", "thought"와 같은 단어들은 모두 "ough"를 포함하지만 발음이 전부 다릅니다. 이는 언어 학습자에게 상당한 혼란과 발음 오류를 야기합니다.

G2P 모듈은 단어의 철자법(자소, grapheme)과 발음(음소, phoneme) 간의 관계를 분석하여 LOGOS가 다음을 수행할 수 있게 합니다:

1. **발음 난이도 예측** - 학습자에게 단어를 제시하기 전에
2. **특정 오류 예측** - 학습자의 모국어를 기반으로
3. **학습 전이 측정** - 철자 패턴 숙달 시

**비즈니스 필요성**: 언어 학습자는 예측 불가능한 철자-발음 관계를 가진 단어를 적절한 지원 없이 접하면 시간을 낭비하고 화석화된(fossilized) 발음 오류를 형성합니다. G2P 패턴을 분석함으로써 LOGOS는 발음 교육을 단계적으로 구성하고, 최적의 순서로 단어를 제시하며, 목표화된 피드백을 제공할 수 있습니다.

**사용 시점**:
- 어휘 태스크 생성 시 단어 난이도 추정
- LanguageObjectVector의 음운론적 및 철자적 차원 계산 시
- 특정 학습자가 어떤 오류를 범할지 예측 시
- 하나의 철자 패턴 숙달이 유사 패턴 학습에 도움이 되는지 측정 시

---

### 미시적 규모: 직접적 관계

#### 의존성 (이 모듈이 필요로 하는 것)

이 모듈은 의도적으로 외부 의존성 없이 자체 완결적으로 구성되어 있습니다:
- 모든 분석에 순수 TypeScript 사용
- G2P 규칙에 정규식 기반 패턴 매칭 사용
- 음절 계산 및 강세 추정에 휴리스틱 알고리즘 사용

**설계 근거**: 외부 음소 사전(CMU Pronouncing Dictionary 등)을 피함으로써 모듈이 가볍고 오프라인에서 작동합니다. 불규칙 단어에 대한 정확도가 다소 낮아지는 trade-off가 있지만, 난이도 추정 목적에는 허용 가능합니다.

#### 피의존성 (이 모듈을 필요로 하는 것)

- `src/core/languageObjectVector.ts`: `toPhonologicalVector()`와 `toOrthographicVector()`를 호출하여 어휘 항목의 음운론적/철자적 차원을 채움
- `src/scheduling/taskCalibration.ts`: `analyzeG2PDifficulty()`를 사용하여 스케줄러용 발음 태스크 난이도 추정
- `src/learning/pronunciationTraining.ts`: `predictMispronunciations()`를 사용하여 L1 특화 발음 피드백 생성
- `src/analytics/transferEffects.ts`: `findG2PTransferCandidates()`와 `measureG2PTransfer()`를 사용하여 G2P 훈련이 어휘 습득에 미치는 영향 추적

#### 데이터 흐름

```
단어 입력
    |
    v
analyzeG2PDifficulty() --> ENGLISH_G2P_RULES와 대조
    |                           |
    |                           v
    |                   예외 단어 식별
    |                   묵음 문자 감지
    |                   이중모음 찾기
    |                   자음군 계산
    |
    v
G2PDifficulty 결과
    |
    +--> difficultyScore (0-1) --> 태스크 보정
    +--> irregularPatterns --> 진단 UI
    +--> syllableCount --> 음운론적 벡터
    |
    v
L1 제공 시: analyzeG2PWithL1()
    |
    v
L1Mispronunciation 예측 --> 발음 피드백
```

---

### 거시적 규모: 시스템 통합

#### 아키텍처 계층

이 모듈은 LOGOS 아키텍처의 **핵심 분석 계층(Core Analysis Layer)**에 위치합니다:

```
계층 1: 사용자 인터페이스 (발음 연습, 피드백 표시)
    |
계층 2: 학습 엔진 (태스크 선택, 피드백 생성)
    |
계층 3: 핵심 분석 <-- G2P 모듈 위치
    |       - 어휘 항목의 언어학적 속성 분석
    |       - 난이도 지표 계산
    |       - 지식 표현용 벡터 생성
    |
계층 4: 데이터 계층 (어휘 데이터베이스, 학습자 프로필)
```

#### 큰 그림에서의 영향

G2P 모듈은 세 가지 주요 LOGOS 기능을 가능하게 합니다:

**1. 지능형 태스크 시퀀싱**
G2P 분석 없이는 LOGOS가 단어를 임의 순서로 제시합니다. G2P 분석을 통해 시스템은 규칙적(예측 가능한 철자)에서 불규칙적 순서로 단어를 배열하여, 학습자가 예외를 만나기 전에 패턴 인식을 구축할 수 있게 합니다.

**2. 개인화된 오류 예측**
스페인어 화자가 영어를 배울 때 범하는 발음 오류는 중국어 화자와 다릅니다. L1 간섭 데이터베이스를 통해 LOGOS는 오류가 발생하기 전에 예측하고 선제적 교육을 제공할 수 있습니다.

**3. 전이 효과 측정**
학습자가 "-tion" 접미사 패턴(nation, station, operation)을 숙달하면 새로운 "-tion" 단어를 더 빨리 배울까요? 전이 효과 함수들은 이를 측정하여, LOGOS가 명시적으로 가르칠 패턴과 학습자가 자연스럽게 발견하도록 둘 패턴을 최적화할 수 있게 합니다.

#### 임계 경로 분석

**중요도 수준**: 높음 (발음 영역)

- **실패 시**: 발음 태스크 난이도 추정이 단순 휴리스틱(단어 길이, 빈도)으로 폴백. L1 특화 피드백 불가. 전이 효과 추적 비활성화.

- **우아한 성능 저하(Graceful degradation)**: 패턴을 찾지 못해도 모듈은 합리적인 기본값을 반환. 미지의 단어도 음절 수와 기본 분석을 받음.

- **단일 실패 지점 없음**: 다른 LOGOS 컴포넌트는 G2P 분석 없이 기능할 수 있지만, 발음 관련 기능이 크게 저하됨.

---

### 기술 개념 (쉬운 설명)

#### 자소-음소 대응 (G2P)
**기술적 정의**: 문자 시퀀스(자소)와 발화 소리 표현(음소) 간의 체계적 매핑으로, 규칙, 맥락, 예외를 포함합니다.

**쉬운 설명**: 철자를 보고 발음하는 방법을 알려주는 규칙입니다. 스페인어에서는 이 규칙이 거의 완벽합니다 - "a"는 항상 "아"로 소리납니다. 영어에서는 이 규칙이 예외로 가득합니다 - "ough"는 "through"에서 "우", "though"에서 "오", "tough"에서 "어프", "thought"에서 "오-"로 소리날 수 있습니다.

**사용 이유**: 이러한 규칙과 예외를 목록화함으로써 어떤 단어가 발음하기 어렵고 왜 그런지 예측할 수 있습니다.

#### L1 간섭 (부정적 전이, Negative Transfer)
**기술적 정의**: 학습자의 모국어(L1)에서 온 음운 패턴이 목표 언어(L2) 발음을 방해하여, 대조 분석으로 예측 가능한 체계적 오류 패턴을 일으킵니다.

**쉬운 설명**: 모국어가 새 언어 발음을 "방해"할 때입니다. 스페인어에는 "sp-"로 시작하는 단어가 없어서 스페인어 화자는 본능적으로 앞에 "e" 소리를 추가하여 "spanish"를 "에스페니쉬"처럼 발음합니다. 무작위가 아닙니다 - 모국어에 어떤 소리가 있는지에 따라 예측 가능합니다.

**사용 이유**: 학습자의 모국어를 알면 정확히 어떤 영어 소리가 어려울지 예측하고 목표화된 연습을 제공할 수 있습니다.

#### 음운론적 벡터 (Phonological Vector)
**기술적 정의**: 음소 시퀀스, 음절 구조, 강세 패턴, 음절 수를 포함한 단어의 소리 속성에 대한 다차원 표현으로, 머신러닝 모델의 입력 특성으로 사용됩니다.

**쉬운 설명**: 단어가 어떻게 들리는지에 대한 숫자 "지문"입니다. 음절이 몇 개인지, 어떤 음절에 강세가 있는지, 자음과 모음의 패턴이 무엇인지 등을 담습니다. 이 지문을 통해 컴퓨터가 두 단어의 소리가 얼마나 유사한지 비교할 수 있습니다 - 컴퓨터가 실제로 "듣지" 않더라도.

**사용 이유**: LanguageObjectVector는 어휘 항목을 완전히 표현하기 위해 음운 정보가 필요합니다. 이 벡터가 그 차원을 제공합니다.

#### 전이 효과 (긍정적 전이, Positive Transfer)
**기술적 정의**: 이전에 숙달한 항목과 구조적 패턴을 공유하는 새 항목의 습득률에서 측정 가능한 향상으로, 명시적 패턴 교육 전후의 학습 곡선 차이로 계산됩니다.

**쉬운 설명**: 하나를 배우면 비슷한 것들을 더 쉽게 배울 때입니다. "nation"에서 "-tion" 어미를 숙달하면 "station", "vacation", "education"을 더 빨리 배웁니다 - 뇌가 이미 그 패턴을 파악했기 때문입니다. 전후 학습 속도를 비교하여 측정합니다.

**사용 이유**: 전이를 측정함으로써 LOGOS는 명시적으로 가르칠 가치가 있는 고가치 패턴과 학습자가 자연스럽게 익히는 패턴을 식별할 수 있습니다.

#### 묵음 문자 (Silent Letters)
**기술적 정의**: 단어의 철자 표현에 나타나지만 발음에서 대응하는 음소가 없는 자소로, 종종 이전 발음이나 차용 철자의 역사적 잔재입니다.

**쉬운 설명**: 쓰지만 말하지 않는 문자입니다. "knife"의 "k", "climb"의 "b", "night"의 "gh". 영어가 수세기 전에는 이 문자들을 발음했지만, 발음이 변하면서 철자는 그대로 유지된 것입니다.

**사용 이유**: 묵음 문자는 발음 난이도의 주요 원인입니다. 이를 감지하면 학습자에게 경고하고 특별 교육을 제공할 수 있습니다.

#### 이중모음 (Vowel Digraph)
**기술적 정의**: 단일 음소나 이중모음을 나타내는 두 모음 자소의 시퀀스로, 단어 기원과 맥락에 따라 가변적 발음을 갖습니다.

**쉬운 설명**: 두 모음이 함께 하나의 소리를 만드는 것입니다. "beat"의 "EA"는 긴 "이" 소리를 냅니다. 하지만 "bread"의 "EA"는 짧은 "에" 소리를 냅니다. 이러한 조합은 항상 일관적이지 않아서 까다롭습니다.

**사용 이유**: 영어의 모음 조합은 예측 불가능하며 G2P 난이도 점수에 크게 기여합니다.

---

### 핵심 데이터 구조

#### ENGLISH_G2P_RULES 데이터베이스

모듈은 범주별로 정리된 40개 이상의 G2P 규칙을 포함합니다:

| 범주 | 패턴 예시 | 음소 | 신뢰도 | 비고 |
|------|----------|------|--------|------|
| Magic E | `a[^aeiou]e$` | /ei/ | 85% | "make"는 해당, "have"는 비해당 |
| 이중모음 | `ee` | /i:/ | 95% | 매우 신뢰할 수 있음 |
| 이중모음 | `ea` | /i:/ | 70% | 예외 많음 |
| R-통제 | `ar` | /a:r/ | 85% | "car"는 해당, "war"는 비해당 |
| 묵음 자음 | `^kn` | /n/ | 99% | k는 항상 묵음 |
| 자음 이중자 | `ph` | /f/ | 99% | 그리스어 기원 |
| 연음 C | `c[eiy]` | /s/ | 95% | "city", "cent" |
| 접미사 | `-tion` | /shun/ | 95% | 매우 신뢰할 수 있음 |
| 의학 | `^psych` | /saik/ | 99% | 도메인 특화 |

**신뢰도 점수**: 규칙이 얼마나 일관되게 적용되는지 나타냅니다. 낮은 신뢰도(예: "ou" 50%)는 예외가 많음을 의미합니다.

#### L1_INTERFERENCE_PATTERNS 데이터베이스

6개 모국어 배경 지원:

| L1 | 주요 간섭 패턴 | 일반 패턴 |
|----|---------------|----------|
| 스페인어 | sp-에 /esp-/, v에 /b/, /th/ 부재 | 모음 약화 어려움 |
| 포르투갈어 | th에 /t/ 또는 /f/, 묵음 h | 비음화 전이 |
| 중국어(만다린) | 접근음 r, v에 /w/, /th/ 부재 | 어말 자음 탈락 |
| 일본어 | r/l 합병, v에 /b/, f에 /h/ | 자음군에 모음 삽입 |
| 한국어 | f에 /p/, v에 /b/, r/l 변이음 | 어말 자음 미파음 |
| 아랍어 | p에 /b/, v에 /f/ | 모음 군집 회피 |

---

### 알고리즘 상세

#### 난이도 점수 계산

`analyzeG2PDifficulty()` 함수는 기여도를 누적하여 0-1 난이도 점수를 계산합니다:

| 요인 | 기여도 | 근거 |
|------|--------|------|
| 예외 단어 일치 | +0.20 | 일반 규칙 위반 단어 |
| 묵음 문자 존재 | +0.15 | 예측 불가능한 발음 |
| 각 모음 조합 | +0.10 | 가변적 발음 |
| 각 3+ 자음군 | +0.15 | 조음 난이도 |
| 3음절 초과 | +0.05/음절 | 길이 복잡성 |
| 불규칙 강세 | +0.10 | 예측 불가능한 강조 |

점수는 최대 1.0으로 제한됩니다.

#### 음절 계산 휴리스틱

`countSyllables()` 함수는 조정이 포함된 모음 계산 접근법을 사용합니다:

1. 모음 그룹 계산 (`[aeiouy]+`)
2. 묵음 어말 "e"에 대해 1 감산 (자음 뒤일 경우)
3. 음절적 "-le" 어미에 대해 1 가산 ("ble", "tle" 등)
4. 음절적 "-ed" 어미에 대해 1 가산 ("t" 또는 "d" 뒤)
5. 최소 1음절 보장

**정확도**: 이 휴리스틱은 일반 영어 어휘에서 약 85% 정확도를 달성합니다. "fire"(악센트에 따라 1 또는 2음절) 같은 경계 사례는 더 단순한 계산을 기본값으로 처리합니다.

---

## G2P-IRT 통합 모듈

> **최종 업데이트**: 2026-01-06
> **코드 위치**: `src/core/g2p-irt.ts`
> **상태**: 활성

---

### 맥락과 목적

이 모듈은 언어 학습의 중요한 측정 문제를 해결하기 위해 존재합니다: **동일한 단어가 맥락에 따라 쉬울 수도 어려울 수도 있을 때 발음 능력을 어떻게 정확히 평가할 것인가?**

"psychology"라는 단어를 생각해 봅시다. 그 난이도는 다음에 따라 극적으로 변합니다:
- **양식(Modality)**: 묵독(쉬움) vs. 소리 내어 발음(더 어려움)
- **태스크 유형**: 올바른 발음 인식(더 쉬움) vs. 산출(더 어려움)
- **시간 압박**: 시간 제한 없는 정확도(더 쉬움) vs. 빠른 유창성(더 어려움)
- **학습자의 모국어**: 한국어 화자는 스페인어 화자보다 초두 "ps-" 자음군에 더 어려움을 겪음
- **음운론적 층위**: 문자 수준 해독 vs. 음절 패턴 vs. 전체 단어 인식

표준 문항 반응 이론(Item Response Theory, IRT)은 난이도를 고정 매개변수로 취급합니다. 하지만 발음 난이도는 **맥락 의존적**입니다. 학습자가 알파벳 해독 기술은 강하지만 음절 패턴 인식은 약할 수 있습니다. 전통적 IRT는 이 미묘함을 가리는 하나의 능력 점수만 제공합니다.

G2P-IRT 통합 모듈은 다음을 구현하여 **언어학적 분석**(G2P)과 **심리측정적 측정**(IRT)을 연결합니다:

1. **맥락 의존적 난이도 매개변수** - 동일 단어가 태스크 맥락에 따라 다른 IRT 난이도를 가짐
2. **다차원 능력 추적** - 다른 기술에 대한 별도 세타 추정(읽기 vs. 말하기, 알파벳 vs. 단어 수준)
3. **L1 특화 조정** - 모국어 전이 효과가 난이도 매개변수 수정
4. **피셔 정보 기반 선택** - 효율적 능력 추정을 위한 최적 문항 선택

**학술적 기반**:
- **형식 인식 IRT (EDM 2022)**: 태스크 형식이 전체 검사 설계뿐 아니라 문항 난이도 매개변수에도 영향을 미친다는 연구
- **다차원 IRT (MIRT)**: 단일 세타 대신 여러 능력 차원을 추정할 수 있는 심리측정 모델
- **Ma, B. et al. (2025)**: 간격 반복을 사용한 개인화 언어 학습 - 발음 학습과 적응형 알고리즘 연결

**비즈니스 필요성**: 맥락 인식 난이도가 없으면 LOGOS는 특정 양식에 너무 쉽거나 너무 어려운 단어를 제시합니다. 학습자가 읽기는 잘하지만 말하기는 어려워할 수 있는데, 두 상황에서 같은 단어 난이도를 받게 됩니다. 이는 연습 시간을 낭비하고 학습자를 좌절시킵니다.

**사용 시점**:
- 학습자의 현재 능력에 최적인 문항을 찾는 적응형 태스크 선택 시
- 발음 연습 후 능력 추정 업데이트 시
- 어떤 음운론적 층위(알파벳, 음절, 단어)를 목표로 교육할지 결정 시
- 잠재적 연습 문항의 예상 학습 이득 계산 시

---

### 미시적 규모: 직접적 관계

#### 의존성 (이 모듈이 필요로 하는 것)

**`src/core/irt.ts`에서:**
- `probability2PL()` - 2모수 로지스틱 모델을 사용한 응답 확률 계산
- `estimateThetaEAP()` - 기대사후추정(Expected A Posteriori) 능력 추정 (베이지안 접근)
- `fisherInformation()` - 문항이 능력에 대해 얼마나 많은 정보를 제공하는지 정량화

**`src/core/g2p.ts`에서:**
- `G2PDifficulty` 타입 - 발음 분석 포함 (불규칙 패턴, 음절 수, 오발음 예측)

**`src/core/types.ts`에서:**
- `TaskType` - 학습 태스크 유형 (인식, 산출, 시간 제한)
- `TaskModality` - 입출력 채널 (시각, 청각, 혼합)
- `ThetaEstimate` - 표준 오차가 포함된 능력 추정
- `ItemParameter` - IRT 매개변수 (변별도, 난이도)

#### 피의존성 (이 모듈을 필요로 하는 것)

- `src/main/services/task-generation.service.ts`: `selectOptimalG2PItem()`을 사용하여 학습 효율을 최대화하는 발음 태스크 선택
- `src/main/services/scoring-update.service.ts`: `updateG2PThetaProfile()`을 사용하여 응답 후 능력 추정 조정
- `src/scheduling/pronunciationScheduler.ts`: `recommendG2PLayer()`를 사용하여 교육 수준 결정
- `src/analytics/phonologicalProgress.ts`: `assessG2PReadiness()`를 사용하여 층위 전반의 학습자 진행 추적

#### 데이터 흐름

```
G2P 분석 (g2p.ts에서)
    |
    v
g2pToIRTParameters() --> 난이도 점수를 로짓 척도로 변환
    |                           |
    |                           v
    |                   층위 임계값 설정
    |                   오발음 예측에서 L1 조정 구축
    |
    v
G2PIRTParameters (기본 난이도 + 맥락 조정)
    |
    +------------+------------+
    |            |            |
    v            v            v
태스크 맥락   사용자 프로필  문항 풀
(양식,        (차원별        (후보
 태스크 유형, 세타)          문항)
 시간 제한, L1)
    |            |            |
    v            v            v
getContextualDifficulty() --> 유효 b 매개변수
    |
    v
selectOptimalG2PItem() --> 피셔 정보 최대화
    |
    v
학습자에 최적 문항 선택
    |
    v
사용자 응답 --> updateG2PThetaProfile()
    |
    v
차원별 업데이트된 세타 추정
```

---

### 거시적 규모: 시스템 통합

#### 아키텍처 계층

이 모듈은 LOGOS 아키텍처의 **세 시스템 교차점**에 위치합니다:

```
계층 1: 사용자 인터페이스 (발음 연습)
    |
계층 2: 학습 엔진 (태스크 선택, 피드백)
    |
계층 3: 적응형 알고리즘 <-- G2P-IRT 통합 위치
    |       - 언어학적 난이도를 심리측정적 매개변수로 변환
    |       - 다중 능력 차원 추적
    |       - 문항 선택 최적화
    |
    +-------+-------+
    |               |
    v               v
계층 4a: G2P       계층 4b: IRT
(언어학적          (심리측정적
 분석)             측정)
    |               |
    v               v
계층 5: 핵심 데이터 (어휘, 학습자 프로필)
```

G2P-IRT 모듈은 언어학적 영역과 심리측정적 영역 사이를 번역하는 **브릿지 모듈**입니다. G2P 분석은 "이 단어는 불규칙 패턴과 묵음 문자가 있습니다"라고 알려줍니다. IRT는 "이 학습자의 능력 세타 = 0.5입니다"라고 알려줍니다. G2P-IRT 모듈은 다음을 답합니다: "이 학습자의 능력과 이 단어의 패턴을 고려할 때, 말하기 태스크에서 올바르게 발음할 확률은?"

#### 큰 그림에서의 영향

**1. 개인화된 난이도 보정**

이 모듈 없이는 LOGOS가 정적 난이도 값을 사용합니다. 이 모듈로 동일 단어가 극적으로 다른 유효 난이도를 가질 수 있습니다:

| 단어 | 읽기 태스크 | 말하기 태스크 | 말하기 (시간 제한) | 한국어 L1 화자 |
|------|------------|--------------|------------------|--------------|
| "psychology" | b = 0.2 | b = 0.8 | b = 1.1 | b = 1.3 |

이를 통해 모든 맥락에서 태스크 난이도를 학습자 능력에 정밀하게 매칭할 수 있습니다.

**2. 계층적 기술 진단**

3층위 모델(알파벳, 음절, 단어)은 학습자가 어디서 어려움을 겪는지 드러냅니다:

- **알파벳 문제**: 학습자가 기본 자소-음소 대응을 숙달하지 못함
- **음절 문제**: 학습자가 문자는 해독하지만 음절 패턴에서 어려움
- **단어 문제**: 학습자가 패턴은 알지만 전체 단어를 유창하게 인식하지 못함

이 진단은 기본 해독 연습이 필요한 학습자에게 고급 단어를 제시하는 대신 적절한 수준으로 교육을 유도합니다.

**3. 효율적 능력 추정**

피셔 정보 기반 문항 선택은 능력을 정확히 추정하는 데 더 적은 연습 문항이 필요함을 의미합니다. 무작위 연습 대신 각 문항이 정보 이득을 최대화하도록 선택되어, 너무 쉽거나 너무 어려운 문항에 낭비되는 시간을 줄입니다.

**4. L1 인식 적응**

학습자 모국어의 전이 문제가 난이도 모델에 내장됩니다. 영어를 연습하는 한국어 화자는 한국어에 존재하는 소리를 포함하는 더 쉬운 문항을 자동으로 받고, 존재하지 않는 소리(/f/, /v/, /th/ 등)에는 적절히 도전적인 문항을 받습니다.

#### 임계 경로 분석

**중요도 수준**: 높음 (적응형 발음 시스템)

- **실패 시**: 발음 태스크가 정적 난이도 값으로 폴백. 다차원 능력 추적 상실 - 학습자가 별도의 읽기/말하기/층위 추정 대신 하나의 결합된 세타를 받음. 문항 선택이 최적화 대신 무작위가 됨.

- **우아한 성능 저하**: 프로필이나 매개변수가 없을 때 모든 함수가 합리적인 기본값 반환. 신규 사용자는 응답과 함께 업데이트되는 0 초기화 프로필을 받음. 누락된 L1 조정은 0(조정 없음)으로 기본값.

- **성능 고려사항**: 피셔 정보 계산은 후보 문항을 반복해야 함. 대규모 문항 풀(1000+ 문항)의 경우 최적화 전에 대략적 난이도 범위로 후보를 사전 필터링 권장.

---

### 기술 개념 (쉬운 설명)

#### 맥락 의존적 난이도 매개변수
**기술적 정의**: 태스크 형식, 양식, 시간 제약, 학습자 특성의 함수로 변하는 IRT의 문항 난이도 매개변수로, 로짓 척도에서 기본 난이도 값에 가산적 조정으로 구현됩니다.

**쉬운 설명**: 단어가 산과 같다고 상상해 보세요. 그 "높이"(난이도)는 고정되어 있지 않습니다 - 어떻게 오르느냐에 달렸습니다. 등산로 걷기(읽기)는 암벽 등반(말하기)보다 쉽습니다. 시간 제한을 추가하면 더 어려워집니다. 익숙하지 않은 장비를 들고 가면(익숙하지 않은 L1 소리) 더 어려워집니다. 이 모듈은 이 모든 요인을 합산하여 "유효 높이"를 계산합니다.

**사용 이유**: 단어당 하나의 난이도 숫자로는 부족합니다. "Psychology"는 묵독으로는 사소하게 쉽지만 시간 압박 하에 올바르게 발음하기는 어렵습니다. 맥락 조정 없이는 쉬운 태스크로 학습자를 지루하게 하거나 불가능한 것으로 좌절시킵니다.

#### 다차원 IRT (MIRT)
**기술적 정의**: 다른 기술 차원에 대해 별도의 능력 매개변수(세타)를 추정하는 단일차원 문항 반응 이론의 확장으로, 문항이 각 차원에 다르게 적재될 수 있습니다.

**쉬운 설명**: "당신의 발음 능력은 10점 만점에 7점입니다"라고 말하는 대신, "당신의 읽기-발음은 8, 말하기-발음은 5, 알파벳 해독은 9, 음절 인식은 6, 전체 단어 유창성은 4입니다"라고 말합니다. 이 상세한 프로필은 강점과 연습이 필요한 곳을 보여줍니다.

**사용 이유**: 발음은 하나의 기술이 아닙니다 - 여러 관련 기술입니다. 개별 문자 해독은 잘하지만 음절 강세 패턴에서 어려움을 겪을 수 있습니다. 단일 세타는 이 구분을 숨깁니다.

#### G2P 층위 계층 (알파벳, 음절, 단어)
**기술적 정의**: 학습자가 개별 문자 해독(알파벳)에서 음절 패턴 인식을 거쳐 전체 단어 철자 표현으로 진행하는 자소-음소 매핑 정밀도 단계를 거치는 읽기 습득의 발달 모델입니다.

**쉬운 설명**: 단어 발음 배우기는 악보 읽기 배우기와 같습니다:
- **알파벳 단계**: 개별 음표를 식별할 수 있음 (문자 = 소리)
- **음절 단계**: 일반적인 음표 패턴과 화음을 인식함 (문자 조합 = 소리 패턴)
- **단어 단계**: 개별 음표를 생각하지 않고 전체 구절을 초견으로 읽음 (전체 단어 = 자동 발음)

대부분의 성인 언어 학습자는 음절과 단어 수준에서 작업이 필요하지만, 일부는 알파벳 복습이 도움이 됩니다.

**사용 이유**: 음절 패턴을 숙달하지 못한 사람에게 단어 수준 유창성을 가르치는 것은 악기 조율을 못하는 사람에게 기타 코드를 가르치는 것과 같습니다. 층위 계층은 교육이 학습자의 현재 수준에 맞도록 보장합니다.

#### 피셔 정보 기반 문항 선택
**기술적 정의**: 현재 능력 추정에서 평가된 피셔 정보 함수를 최대화하는 다음 문항을 선택하여, 가능한 적은 문항으로 세타 추정의 표준 오차를 최소화하는 적응형 검사 전략입니다.

**쉬운 설명**: 줄자 없이 누군가의 키를 알아내려 한다고 상상해 보세요. "5피트보다 큽니까?"라고 물을 수 있습니다. 예라면 "6피트보다 큽니까?" 아니라면 "5피트 6인치보다 큽니까?" 각 질문은 불확실성을 절반으로 줄입니다. 피셔 정보는 우리가 이미 그 사람에 대해 아는 것을 기반으로 어떤 질문(어떤 연습 문항)이 불확실성을 가장 많이 줄일지 알려줍니다.

**사용 이유**: 효율적 평가는 더 빠른 학습을 의미합니다. 학습자가 중급 수준임을 이미 안다면 초급 단어를 보여주는 것은 그들의 능력에 대해 아무것도 알려주지 않습니다. 피셔 정보 최대화는 모든 연습 문항이 최대 진단 가치를 제공하도록 보장합니다.

#### L1 특화 난이도 조정
**기술적 정의**: 학습자의 모국어(L1)와 목표 언어(L2) 간의 대조적 음운 분석을 기반으로 한 문항 난이도 매개변수의 가산적 수정으로, 예측되는 전이 간섭을 반영합니다.

**쉬운 설명**: 모국어는 새 소리에 도움이 되거나 방해가 되는 "발음 습관"을 만듭니다. 일본어 화자는 자연스럽게 "r" vs "l"에 어려움을 겪습니다 - 일본어에는 둘 다 없고 중간 소리가 있기 때문입니다. 한국어 화자는 "f" 소리에 어려움을 겪습니다 - 한국어에 없기 때문입니다. 우리는 이러한 어려움을 예측하고 그에 따라 단어 난이도를 조정합니다 - "f"가 있는 단어는 스페인어 화자보다 한국어 화자에게 더 어렵습니다.

**사용 이유**: 모든 학습자를 같게 취급하면 발음 난이도의 주요 요인을 무시합니다. L1 조정으로 LOGOS는 언어적 배경과 관계없이 동등하게 적절한 도전을 제공할 수 있습니다.

#### EAP 유사 업데이트를 통한 세타 추정
**기술적 정의**: 기대사후추정을 근사하는 가중 예측 오차 업데이트를 사용한 점진적 능력 추정으로, 각 응답 후 전체 사후 계산 없이 온라인 학습을 가능하게 합니다.

**쉬운 설명**: 단어를 발음해 본 후, 당신의 능력 추정을 업데이트합니다. 맞혔고 어려웠다면 점수가 많이 오릅니다. 틀렸고 쉬웠다면 점수가 내려갑니다. 코치가 선수를 평가하는 방식과 같습니다 - 쉬운 슛 하나 놓치면 걱정되고, 어려운 슛 하나 놓치면 예상대로입니다.

**사용 이유**: 매 응답 후 전체 베이지안 추정은 계산 비용이 많이 듭니다. 이러한 경량 업데이트는 시스템을 반응적으로 유지하면서 좋은 근사를 제공합니다.

---

### 핵심 데이터 구조

#### G2PIRTParameters

G2P 언어학적 분석과 IRT 심리측정적 매개변수를 결합하는 핵심 구조:

| 필드 | 타입 | 설명 |
|------|------|------|
| `id` | string | 고유 문항 식별자 |
| `content` | string | 단어 또는 패턴 |
| `baseDifficulty` | number | 로짓 척도의 IRT b 매개변수 |
| `discrimination` | number | IRT a 매개변수 (문항이 능력을 얼마나 잘 구분하는지) |
| `guessing` | number | IRT c 매개변수 (MCQ 태스크용) |
| `contextAdjustments` | object | 양식, 태스크 유형, 시간 제한, 층위별 수정자 |
| `layerThresholds` | object | 각 G2P 층위에서 필요한 최소 세타 |
| `l1Adjustments` | Record | 언어별 난이도 수정자 |
| `g2pAnalysis` | G2PDifficulty | 원본 언어학적 분석 |

#### G2PThetaProfile

별도 세타를 추적하는 다차원 능력 프로필:

**전체:**
- `thetaPhonological` - 전역 음운론적 능력

**층위별:**
- `thetaAlphabetic` - 문자-소리 대응
- `thetaSyllable` - 음절 패턴 인식
- `thetaWord` - 전체 단어 인식

**양식별:**
- `thetaReading` - 시각/묵독 발음
- `thetaListening` - 청각 이해
- `thetaSpeaking` - 구어 산출
- `thetaWriting` - 철자 산출

각 세타는 더 많은 응답과 함께 감소하는 연관된 표준 오차를 갖습니다.

#### DEFAULT_CONTEXT_ADJUSTMENTS

경험적으로 도출된 난이도 수정자:

| 맥락 | 조정 | 근거 |
|------|------|------|
| **양식** | | |
| 읽기 | +0.0 | 기준선 |
| 듣기 | +0.3 | 청각 처리가 부하 추가 |
| 말하기 | +0.6 | 산출이 수용보다 어려움 |
| 쓰기 | +0.4 | 철자 산출이 중간 정도 더 어려움 |
| **태스크 유형** | | |
| 인식 | +0.0 | 기준선 |
| 산출 | +0.5 | 생성이 선택보다 어려움 |
| **시간 제한** | | |
| 무제한 | +0.0 | 기준선 |
| 시간 제한 | +0.3 | 시간 압박이 난이도 추가 |
| **층위** | | |
| 알파벳 | +0.0 | 기준선 (가장 단순한 수준) |
| 음절 | +0.2 | 패턴 복잡성 |
| 단어 | +0.4 | 전체 통합 필요 |

#### L1_DIFFICULTY_ADJUSTMENTS

언어별 전이 난이도:

| L1 | 패턴 | 조정 | 이유 |
|----|------|------|------|
| 한국어 | th_sound | +0.5 | /th/가 한국어 목록에 없음 |
| 한국어 | r_l_distinction | +0.4 | 변이음, 음소가 아님 |
| 한국어 | final_consonant_clusters | +0.3 | 한국어 음절이 단순하게 끝남 |
| 일본어 | r_l_distinction | +0.5 | 일본어에서 합병 |
| 일본어 | v_b_distinction | +0.3 | /v/가 일본어에 없음 |
| 중국어 | final_consonants | +0.4 | 만다린이 허용하는 어말 C가 적음 |
| 중국어 | consonant_clusters | +0.4 | 만다린에서 자음군 드묾 |
| 스페인어 | short_long_vowels | +0.3 | 스페인어에 장단 대비 없음 |
| 스페인어 | initial_s_clusters | +0.3 | 스페인어가 삽입 모음 필요 |

---

### 알고리즘 상세

#### 난이도 변환 (G2P에서 IRT로)

`g2pToIRTParameters()` 함수는 G2P 난이도(0-1 척도)를 IRT 로짓 척도로 변환합니다:

1. 난이도를 [0.01, 0.99]로 **클램프**하여 무한대 방지
2. **로짓 변환**: `b = ln(difficulty / (1 - difficulty))`
3. **층위 임계값 설정**:
   - 알파벳: `baseDifficulty - 1.0`
   - 음절: `baseDifficulty - 0.5` (+ 음절 수 조정)
   - 단어: `baseDifficulty` (+ 불규칙 패턴 조정)
4. 오발음 예측에서 **L1 조정 구축**

#### 맥락적 난이도 계산

`getContextualDifficulty()` 함수는 유효 난이도를 계산합니다:

```
effective_b = base_b
            + modality_adjustment
            + task_type_adjustment
            + timing_adjustment
            + layer_adjustment (지정된 경우)
            + l1_adjustment (해당되는 경우)
```

모든 조정은 로짓 척도에서 가산적입니다. +0.5 조정은 대략 실패 확률을 두 배로 합니다.

#### 맥락에 대한 세타 선택

`selectThetaForContext()` 함수는 여러 세타 추정을 결합합니다:

1. 전체 음운론적 세타로 시작 (100%)
2. 양식별 세타 방향으로 가중 (40%)
3. 층위별 세타 방향으로 가중 (30%, 층위 지정 시)

이는 특정 태스크 맥락에 적절한 혼합 추정을 생성합니다.

#### 프로필 업데이트 알고리즘

`updateG2PThetaProfile()` 함수는 오차 기반 학습을 사용합니다:

1. 정답 응답의 기대 확률 계산
2. 예측 오차 계산: `error = actual - expected`
3. 관련 세타 업데이트: `theta_new = theta_old + learning_rate * error * discrimination`
4. 전체 음운론적 세타를 가중 평균으로 업데이트
5. 응답 수에 따라 표준 오차 감소

학습률(0.1)은 반응성과 안정성 사이에서 균형을 잡습니다.

#### 최적 문항 선택

`selectOptimalG2PItem()` 함수는 피셔 정보를 최대화합니다:

1. 학습자에 대한 맥락 적절 세타 계산
2. 각 후보 문항에 대해:
   - 맥락 조정 난이도 계산
   - 피셔 정보 계산: `I = a^2 * P * (1-P)`
3. 최대 정보를 가진 문항 선택

문항 난이도가 학습자 능력과 일치할 때(P = 0.5) 정보가 최대화됩니다.

---

### 사용 패턴

#### 신규 사용자 초기화

```typescript
const profile = createInitialG2PThetaProfile(userId);
// 모든 세타 0에서 시작, 모든 SE 1.5에서 시작 (높은 불확실성)
```

#### G2P 분석을 IRT 매개변수로 변환

```typescript
const g2pAnalysis = analyzeG2PDifficulty('psychology', 'medical');
const irtParams = g2pToIRTParameters(g2pAnalysis, 1.2); // 변별도 = 1.2
```

#### 최적 연습 문항 선택

```typescript
const context: G2PTaskContext = {
  modality: 'speaking',
  taskType: 'production',
  isTimed: true,
  targetLayer: 'word',
  userL1: 'korean'
};

const result = selectOptimalG2PItem(candidateItems, userProfile, context);
// result.item = 이 맥락에서 이 학습자에게 최적 문항
// result.information = 예상 정보 이득
```

#### 응답 후 프로필 업데이트

```typescript
const response: G2PResponse = {
  itemId: 'word_123',
  correct: true,
  responseTimeMs: 2500,
  context: taskContext,
  itemParams: itemIRTParams
};

const updatedProfile = updateG2PThetaProfile(userProfile, response);
```

#### 단어 준비도 평가

```typescript
const readiness = assessG2PReadiness(userProfile, wordParams);
// readiness.ready: boolean - 단어 수준 시도 가능?
// readiness.recommendedLayer: 'alphabetic' | 'syllable' | 'word'
// readiness.confidenceLevel: 'low' | 'medium' | 'high'
```

---

## 형태론적 분석 모듈

> **최종 업데이트**: 2026-01-04
> **코드 위치**: `src/core/morphology.ts`
> **상태**: 활성
> **이론적 기반**: ALGORITHMIC-FOUNDATIONS.md Part 6.1, THEORETICAL-FOUNDATIONS.md Section 2.2

---

### 맥락과 목적

#### 이 모듈이 존재하는 이유

형태론적 분석 모듈은 다음을 답합니다: **"이 단어는 어떻게 구성되어 있고, 그 조각들에서 무엇을 배울 수 있는가?"**

의학 맥락에서 "contraindication"이라는 단어를 생각해 봅시다. 다음을 아는 학습자:
- "contra-"는 "반대"를 의미
- "-tion"은 동사에서 명사를 만듦
- "indicate"가 어근

...는 이 정확한 단어를 전에 보지 않고도 의미를 추론할 수 있습니다. 이것이 **전이 효과(Transfer Effect)**입니다: 단어 부분에 대한 훈련이 새 단어 학습을 가속화합니다.

**비즈니스 필요성**: LOGOS는 효율적으로 어휘를 가르치는 것을 목표로 합니다. 모든 단어를 고립된 것으로 취급하는 대신, 형태론적 분석은 다음을 가능하게 합니다:
1. 공유 접사로 단어 그룹화 ("un-"을 한 번 가르치고 수백 단어에 적용)
2. 전이 측정 ("pre-" 학습이 "predict", "prevent", "precaution"에 도움이 되었나?)
3. 난이도 추정 (희귀 접사가 있는 단어가 더 어려움)
4. LanguageObjectVector의 형태론적 구성요소 생성

#### 사용 시점

- **LanguageObjectVector 생성**: 모든 어휘 항목이 `morphological` 속성에 대해 분석됨
- **태스크 생성**: 형태소 수와 접사 생산성에 따라 난이도 보정
- **병목 감지**: 형태론(MORPH 구성요소)이 연쇄 오류를 일으키는지 식별
- **전이 측정**: 접사 훈련이 새 단어 추론을 개선하는지 추적
- **스캐폴딩 결정**: 투명한 형태론을 가진 단어는 불투명한 것과 다른 힌트를 받음

---

### 미시적 규모: 직접적 관계

#### 의존성 (이 모듈이 필요로 하는 것)

이 모듈은 외부 의존성 없이 **자체 완결적**입니다:
- 내장 접사 데이터베이스 (`ENGLISH_PREFIXES`, `ENGLISH_SUFFIXES`)
- 불규칙 형태 테이블 (`IRREGULAR_PAST`, `IRREGULAR_PLURAL`)
- 모든 분석 함수가 외부 라이브러리 없이 순수 TypeScript

#### 피의존성 (이 모듈을 필요로 하는 것)

| 파일 | 사용 |
|------|------|
| `src/core/bottleneck.ts` | 연쇄 분석에 구성요소 유형 'MORPH' 사용; 패턴 추출이 형태론적 오류 패턴 참조 |
| `src/core/types.ts` | 이 모듈이 구현하는 `MorphologicalAnalysis` 및 `Affix` 인터페이스 정의 |
| `src/shared/types.ts` | IPC 통신용 형태론 관련 타입 재내보내기 |
| `src/main/services/claude.ts` | (예상) 어휘 추출 요청에 형태론적 분석 사용 |
| 태스크 생성 시스템 | (예상) `difficultyScore`를 사용하여 태스크 난이도 보정 |

#### 데이터 흐름

```
단어 입력 (예: "unhappiness")
    |
    v
analyzeMorphology()
    |
    +---> 접두사 감지: ENGLISH_PREFIXES와 최장 일치 우선 대조
    |         결과: [{ form: 'un-', meaning: 'not, opposite', productivity: 0.9 }]
    |
    +---> 접미사 감지: ENGLISH_SUFFIXES와 최장 일치 우선 대조
    |         결과: [{ form: '-ness', meaning: 'state/quality', productivity: 0.9 }]
    |
    +---> 어근 추출: "happi" (접사 제거 후 나머지)
    |
    +---> 굴절 감지: 'base' (굴절 표지 없음)
    |
    +---> 파생 유형: 'complex' (접두사와 접미사 모두)
    |
    +---> 난이도 계산: 접사 생산성과 파생 유형 기반
    |
    v
MorphologicalAnalysis 결과
    |
    v
toMorphologicalVector()
    |
    +---> 투명성: 부분에서 의미가 얼마나 예측 가능한지 (이 단어는 0.7)
    |
    +---> 생산성: 평균 접사 생산성 (0.9)
    |
    +---> 굴절 패러다임: "prefix:un-|suffix:-ness|inflection:base|type:complex"
    |
    v
MorphologicalVector (LanguageObjectVector.morphological용)
```

---

### 거시적 규모: 시스템 통합

#### 아키텍처 역할

이 모듈은 LOGOS 아키텍처의 **언어 분석 계층**에 위치합니다:

```
계층 1: 사용자 인터페이스 (React)
    |
계층 2: IPC 통신 (Electron)
    |
계층 3: 핵심 알고리즘 <-- morphology.ts 위치
    |     |- irt.ts (능력 추정)
    |     |- fsrs.ts (간격)
    |     |- pmi.ts (연어)
    |     |- morphology.ts (단어 구조) <-- 여기
    |     |- bottleneck.ts (형태론 사용)
    |     +- priority.ts (스케줄링)
    |
계층 4: 데이터베이스 (Prisma/SQLite)
```

#### 큰 그림에서의 영향

형태론적 분석은 세 가지 중요한 LOGOS 기능을 가능하게 합니다:

**1. 전이 학습 측정**

THEORETICAL-FOUNDATIONS.md의 전이 효과는 하나를 학습하면 관련 항목의 성능이 향상되는지 측정하는 LOGOS의 능력입니다. 형태론의 경우:
- "pre-"가 있는 단어로 사용자 훈련 (preview, predict, precaution)
- 본 적 없는 새 "pre-" 단어로 테스트 (prepayment, preapproval)
- 측정: 접사 훈련이 전이되었는가?

`findTransferCandidates()`와 `measureTransferEffect()` 함수가 이를 구현합니다.

**2. LanguageObjectVector 생성**

LOGOS의 모든 어휘 항목은 다차원 벡터 표현을 갖습니다:
```
LanguageObjectVector = {
    orthographic: ...,
    phonological: ...,
    morphological: <-- 이 모듈이 제공
    syntactic: ...,
    semantic: ...,
    pragmatic: ...
}
```

`morphological` 구성요소는 다음을 포함합니다:
- 어근/어간 식별
- 접두사 및 접미사 체인
- 굴절 패러다임 분류
- 투명성 점수 (부분에서 의미 예측 가능성)
- 생산성 점수 (접사가 얼마나 자유롭게 결합하는지)

**3. 난이도 추정**

태스크 난이도는 형태론적 복잡성에 크게 의존합니다:

| 파생 유형 | 기본 난이도 | 예시 |
|----------|------------|------|
| simple | 0.1 | "cat" |
| derived | 0.3 | "unhappy" |
| compound | 0.4 | "toothbrush" |
| complex | 0.5 | "unacceptable" |

낮은 생산성 접사는 난이도를 추가합니다 (희귀 패턴은 인식하기 어려움).

#### 임계 경로 분석

**중요도 수준**: 중상

- **실패 시**: LanguageObjectVector가 불완전하고, 전이 효과가 측정되지 않으며, 난이도 추정이 덜 정확해짐
- **폴백 동작**: 형태론적 분석 없이도 단어를 학습할 수 있지만 효율성이 저하됨
- **직접적 사용자 대면 실패 없음**: 사용자는 오류를 보지 않지만 학습 최적화가 저하됨
- **병목 감지 저하**: MORPH 구성요소 오류가 다른 구성요소로 잘못 귀속될 수 있음

---

### 기술 개념 (쉬운 설명)

#### 접사 생산성 (Affix Productivity)
**기술적 정의**: 접사가 언어에서 새 어근과 얼마나 자유롭게 결합하는지를 나타내는 생산성 값(0-1)으로, 유형 빈도와 신조어율에 기반합니다.

**쉬운 설명**: 이 단어 부분이 얼마나 "생성적"인가? 접두사 "un-"은 매우 생산적입니다(0.9) --- "unhappy", "unclear", "unfair"라고 말할 수 있고 "uninstall"처럼 새 단어도 만들 수 있습니다. 하지만 "circum-"은 낮은 생산성(0.5)입니다 --- 아무 단어에나 붙일 수 없습니다.

**사용 이유**: 높은 생산성 접사는 어디에나 나타나므로 배우기 쉽습니다. 낮은 생산성 접사는 더 명시적인 교육이 필요합니다.

#### 의미적 투명성 (Semantic Transparency)
**기술적 정의**: 단어의 의미가 형태론적 부분에서 얼마나 구성적으로 예측 가능한지 측정하는 투명성 점수(0-1)입니다.

**쉬운 설명**: 조각을 알면 단어 의미를 추측할 수 있나요? "Unhappy"는 투명합니다 --- 분명히 "happy가 아닌"을 의미합니다. 하지만 "understand"는 불투명합니다 --- "under"나 "standing"과 아무 관련이 없습니다.

**사용 이유**: 투명한 단어는 다르게 스캐폴딩할 수 있습니다 ("'un-'은 무슨 뜻이지? 'happy'는? 그럼 'unhappy'는?"). 불투명한 단어는 전체적 암기가 필요합니다.

#### 파생 유형 (Derivation Type)
**기술적 정의**: 단어 형성 과정의 분류: simple(단일형태소), derived(어근 + 접사), compound(어근 + 어근), complex(다중 접사).

**쉬운 설명**: 이 단어는 어떻게 만들어졌나요?
- **Simple**: 하나의 조각, "cat"이나 "run"처럼
- **Derived**: 어근에 무언가 추가, "un+happy"나 "teach+er"처럼
- **Compound**: 두 단어가 붙음, "tooth+brush"처럼
- **Complex**: 여러 조각이 층을 이룸, "un+accept+able"처럼

**사용 이유**: 복잡한 단어는 더 많은 처리 시간이 필요하고 학습을 위해 구성요소로 분해해야 할 수 있습니다.

#### 굴절 vs. 파생 (Inflection vs. Derivation)
**기술적 정의**: 굴절은 새 어휘소를 만들지 않고 문법적 속성을 변경합니다(run/runs/ran). 파생은 잠재적으로 다른 의미나 품사를 가진 새 어휘소를 만듭니다(run/runner).

**쉬운 설명**:
- **굴절** = 같은 단어, 다른 형태 (walk/walking/walked는 모두 같은 단어)
- **파생** = 새 단어 생성 (walk -> walker는 새 의미를 가진 새 단어)

**사용 이유**: 굴절 오류는 형태론 병목을 시사합니다. 파생 패턴은 전이 학습을 가능하게 합니다.

#### 표제어 추출 (Lemmatization)
**기술적 정의**: 굴절된 형태를 사전 표제어(lemma)로 환원. "running" -> "run", "better" -> "good".

**쉬운 설명**: 사전에서 찾을 "기본 단어"를 찾는 것. "children"을 보면 lemma는 "child"입니다. "went"를 보면 lemma는 "go"입니다.

**사용 이유**: 단어의 모든 형태를 연결하여 "go"를 배우면 "went", "going", "gone"에 도움이 됩니다.

---

### 핵심 함수 설명

#### analyzeMorphology(word, domain?)

**목적**: 단어의 완전한 형태론적 분해.

**과정**:
1. 입력 정규화 (소문자, 트림)
2. 접두사 최장 우선 확인 ("un"이 "under"보다 먼저 매칭되는 것 방지)
3. 접미사 최장 우선 확인 ("-tion"이 "-ation"보다 먼저 매칭되는 것 방지)
4. 나머지 어근 추출
5. 굴절 유형 감지 (base, past, plural 등)
6. 파생 유형 분류
7. 난이도 점수 계산

**반환**: word, root, prefixes[], suffixes[], inflection, derivationType, morphemeCount, difficultyScore를 포함한 `MorphologicalAnalysis`

#### toMorphologicalVector(word, domain?)

**목적**: LanguageObjectVector용 형태론적 구성요소 생성.

**추가 계산**:
- **투명성**: 부분에서 의미가 얼마나 예측 가능한지
- **생산성**: 평균 접사 생산성
- **굴절 패러다임**: 형태론적 구조의 문자열 인코딩

#### findTransferCandidates(trainedWords, candidateWords, domain?)

**목적**: 이미 훈련된 단어와 접사를 공유하는 새 단어 찾기.

**사용 사례**: 사용자가 "predict", "prevent", "precaution"을 연습했습니다. 이제 인식할 수 있을 다른 "pre-" 단어 찾기: "prepayment", "preapproval", "preemptive".

**반환**: 전이 잠재력(훈련이 도움될 가능성)으로 순위가 매겨진 목록.

#### measureTransferEffect(trainedAffixes, testResults)

**목적**: 접사 훈련이 실제로 새 단어 성능을 개선했는지 측정.

**입력**:
- 어떤 접사가 훈련되었는지
- 테스트 결과: {word, correctBefore, correctAfter}

**반환**: accuracyBefore, accuracyAfter, transferGain을 포함한 `MorphologicalTransfer`

---

### 접사 데이터베이스 설계

#### 접두사 범주

| 범주 | 예시 | 의미 패턴 |
|------|------|----------|
| 부정/반대 | un-, in-, dis-, non-, anti- | "~이 아닌", "반대", "대항" |
| 시간/순서 | pre-, post-, ex-, neo- | "전에", "후에", "이전의" |
| 정도/크기 | super-, sub-, over-, under-, hyper- | "위", "아래", "과도한" |
| 수량 | mono-, bi-, tri-, multi-, poly- | "하나", "둘", "셋", "많은" |
| 방향/위치 | inter-, intra-, trans-, extra-, circum- | "사이에", "내부에", "가로질러" |
| 방식 | re-, mis-, co-, auto- | "다시", "잘못", "함께", "스스로" |
| 의학 | cardio-, neuro-, gastro-, hemo- | 신체 부위 특화 |

#### 접미사 범주

| 범주 | 예시 | 생성물 |
|------|------|-------|
| 명사화 | -tion, -ment, -ness, -ity, -er | 동사/형용사에서 명사 |
| 형용사화 | -ful, -less, -able, -ous, -ive | 명사/동사에서 형용사 |
| 동사화 | -ize, -ify, -ate, -en | 명사/형용사에서 동사 |
| 부사화 | -ly, -ward, -wise | 형용사에서 부사 |
| 의학 | -itis, -osis, -ectomy, -ology | 상태/시술 용어 |

#### 도메인 필터링

각 접사는 선택적 `domains` 배열을 가집니다:
- `general`: 모든 맥락에 적용
- `medical`: 전문 의학 용어
- `academic`: 공식/학술적 사용
- `business`: 전문/상업적 맥락
- `technical`: 기술/엔지니어링 맥락

도메인으로 분석할 때 접사가 관련 항목으로 필터링됩니다.

---

### 병목 감지와의 통합

병목 모듈(Part 7)은 연쇄에서 형태론을 MORPH 구성요소로 사용합니다:

```
PHON -> MORPH -> LEX -> SYNT -> PRAG
         ^
         |
   여기서 형태론 오류가 하류 LEX와 SYNT 오류를 유발
```

**오류 패턴 감지** (bottleneck.ts에서):
```typescript
case 'MORPH':
  if (content.match(/ing$/)) return '-ing endings';
  if (content.match(/ed$/)) return '-ed endings';
  if (content.match(/s$/)) return 'plurals/3rd person';
  if (content.match(/tion$/)) return '-tion nominalizations';
  return 'other word forms';
```

사용자가 여러 태스크에서 "-ing" 어미에 어려움을 겪을 때, 병목 시스템은:
1. 높은 MORPH 오류율 감지
2. LEX와 SYNT도 상승했는지 확인 (연쇄 패턴)
3. 권장: "형태론(단어 형태)에 집중하세요. 특히 연습: -ing 어미."

---

### 변경 이력

#### 2026-01-04 - 초기 구현
- **변경 내용**: 완전한 형태론적 분석 모듈 생성
- **이유**: 전이 학습 측정 및 LanguageObjectVector.morphological 생성 가능하게 함
- **영향**: 형태론 인식 어휘 학습의 기반

#### 구현된 기능
- 50개 이상의 영어 접두사와 접미사로 접사 감지
- 도메인별 접사 필터링 (의학, 학술 등)
- 불규칙 형태 처리 (불규칙 과거형, 불규칙 복수형)
- LanguageObjectVector용 형태론적 벡터 생성
- 전이 후보 식별
- 전이 효과 측정
- 복잡성/난이도 점수화
- 표제어 추출 (단순화)

#### 향후 고려사항
- 다국어 접사 데이터베이스 (현재 영어만)
- 경계 사례용 머신러닝 기반 접사 인식
- 형태음운 규칙용 발음 분석과 통합
- 복합어 분리 (현재 기본 패턴 매칭)
