# 핵심 최적화 모듈 (Core Optimization Modules)

> **최종 업데이트**: 2026-01-07
> **원본 위치**: `docs/narrative/src/core/`
> **상태**: Active

이 문서는 LOGOS의 핵심 최적화 알고리즘을 설명합니다. 우선순위 계산(Priority), L1-L2 전이(Transfer), 그리고 이 둘의 통합 모듈을 다룹니다.

---

## 우선순위 계산 모듈 (Priority Calculation Module)

> **코드 위치**: `src/core/priority.ts`

---

### 이 모듈이 존재하는 이유

우선순위(Priority) 모듈은 적응형 학습의 근본적인 질문에 답합니다: **"학습자가 다음에 무엇을 공부해야 하는가?"** 원칙에 기반한 우선순위 지정이 없으면, 학습자는 이미 알고 있는 내용에 시간을 낭비하거나 자신의 수준을 훨씬 넘어서는 콘텐츠에 고전하게 됩니다. 이 모듈은 FRE(Frequency, Relational density, contextual contribution) 우선순위 시스템을 구현하여, 학습하기에 가장 유용한 것(높은 빈도, 높은 연결성)과 학습 가능한 것(난이도, L1에서의 전이, 학습자 능력을 고려)의 균형을 맞춥니다.

핵심 통찰은 우선순위가 비율이라는 것입니다: **가치(Value) / 비용(Cost)**. 배우기 쉬운 고가치 항목이 먼저 스케줄링되고, 어려운 저가치 항목은 나중으로 미뤄집니다.

---

### 핵심 개념

- **FRE 지표**: 어휘 가치의 세 가지 차원:
  - **F (Frequency, 빈도)**: 대상 텍스트에서 단어가 얼마나 자주 나타나는지 (커버리지 가치)
  - **R (Relational Density, 관계 밀도)**: 단어가 다른 어휘와 얼마나 연결되어 있는지 (허브 단어)
  - **E (contextual contribution, 맥락적 기여)**: 의미 이해에서 단어가 얼마나 중요한지

- **비용 요소 (Cost Factors)**: 학습을 더 어렵게 또는 쉽게 만드는 요소:
  - **기본 난이도 (Base Difficulty)**: 로짓 척도(logit scale)의 IRT 기반 난이도
  - **전이 이득 (Transfer Gain)**: L1-L2 유사성(동족어, 공유 구조)으로부터의 이점
  - **노출 필요성 (Exposure Need)**: 학습자 능력과 항목 난이도 간의 격차

- **긴급도 (Urgency)**: 간격 반복(Spaced Repetition) 통합 - 항목이 복습 날짜에 가까워지거나 지나면 긴급해짐

- **강화된 우선순위 (Enhanced Priority)**: 다음을 포함하는 확장 모델:
  - 화용적 복잡성 (Pragmatic complexity) - 어체, 공손성
  - 형태론적 복잡성 (Morphological complexity) - 단어 형태, 굴절
  - 음운론적 난이도 (Phonological difficulty) - 발음 어려움
  - 도메인 관련성 (Domain relevance) - 의료 영어, 비즈니스 영어

---

### 설계 결정

#### 우선순위 = FRE / 비용

공식 `Priority = (w_F * F + w_R * R + w_E * E) / Cost`는 가치와 노력의 균형을 우아하게 맞춥니다. 배우기 상대적으로 쉬운(L1 전이 덕분이거나 학습자 수준에 맞는) 많은 연결을 가진 고빈도 단어가 상위로 올라갑니다. 이는 시스템이 쉬운 단어만 반복하거나(낮은 학습 이득) 어려운 단어만 반복하는(좌절감과 비효율) 것을 방지합니다.

#### 수준별 적응형 가중치 조정

초보자는 `{f: 0.5, r: 0.25, e: 0.25}`를 받습니다 - 어휘 커버리지가 병목이므로 빈도에 크게 가중됩니다. 고급 학습자는 `{f: 0.3, r: 0.3, e: 0.4}`를 받습니다 - 핵심 어휘를 이미 갖추고 있으므로 맥락적 뉘앙스에 더 많은 가중치가 부여됩니다.

#### 긴급도는 덧셈이 아닌 곱셈

최종 점수는 `Priority + Urgency`가 아닌 `Priority * (1 + Urgency)`입니다. 이는 기한이 지난 고우선순위 항목이 지배하도록 하면서, 약간 기한이 지난 저우선순위 항목이 고우선순위 새 항목을 앞지르지 않도록 합니다. 곱셈 관계는 기한 지난 항목을 부스트하면서 상대적 우선순위를 유지합니다.

#### 도메인 부스트는 승수

사용자가 특정 도메인(예: 의료 영어)에 집중할 때, 높은 도메인 관련성을 가진 항목은 최대 50% 우선순위 부스트를 받습니다. 이는 오버라이드가 아닌 승수이므로, 도메인 관련 항목은 임의로 승격되지 않고 여전히 실력으로 경쟁합니다.

#### 전이 이득은 비용 감소

L1-L2 전이(동족어, 유사한 문법)는 우선순위를 직접 높이는 것이 아니라 학습 비용을 줄입니다. 이는 유사한 항목이 배우기 *더 쉽다*는 현실을 반영하며, 본질적으로 더 *가치 있는* 것은 아닙니다. 영어와 유사한 한국어 외래어는 배우기 쉬울 수 있지만, 그것이 고빈도 비동족어보다 더 중요하다는 것을 의미하지는 않습니다.

---

### 통합 지점

#### 의존성 (이 모듈이 필요로 하는 것)

- **`./pragmatics`**: `calculatePragmaticDifficulty()`, `PragmaticProfile` - 화용적 복잡성 점수화용
- **`./transfer`**: `calculateTransferGain()`, `getTransferCoefficients()` - L1-L2 전이 이점 계산용

#### 종속 모듈 (이 모듈을 필요로 하는 것)

- **`src/main/services/state-priority.service.ts`**: `buildLearningQueue()` 및 `buildEnhancedLearningQueue()`를 사용하여 학습 큐 구축
- **`src/main/services/session.service.ts`**: `getSessionItems()`를 사용하여 연습 세션 구성
- **`src/core/index.ts`**: 편리한 임포트를 위해 모든 우선순위 함수 재내보내기

#### 데이터 흐름

```
사용자 상태 (theta, weights, L1)
         |
         v
언어 객체 (FRE 지표, IRT 난이도 포함)
         |
         +---> computeFRE() --> 가중 가치 점수
         |
         +---> estimateCostFactors() --> 기본 난이도, 전이, 노출 필요성
         |          |
         |          +---> computeCost() --> 비용 분모
         |
         +---> computePriority() --> FRE / Cost
                   |
                   v
숙달 맵 (다음 복습 날짜, 단계)
         |
         +---> computeUrgency() --> 기한 초과 승수
         |
         +---> computeFinalScore() --> Priority * (1 + Urgency)
                   |
                   v
buildLearningQueue() --> 정렬된 QueueItem[] (최고 우선)
         |
         v
getSessionItems() --> 기한 + 새 항목의 균형 잡힌 세션
```

---

### 주요 함수

| 함수 | 목적 |
|------|------|
| `computeFRE(metrics, weights)` | F, R, E 값의 가중 합계 |
| `computeCost(factors)` | Base - Transfer + Exposure need |
| `computePriority(object, userState)` | 전체 우선순위 계산 (FRE / Cost) |
| `computeUrgency(nextReview, now)` | 간격 반복 긴급도 (0 ~ 3) |
| `buildLearningQueue(objects, userState, masteryMap, now)` | 최종 점수로 정렬된 전체 큐 |
| `getSessionItems(queue, size, newRatio)` | 균형 잡힌 세션 추출 |
| `computeEnhancedPriority(object, userState, config)` | 화용론 및 복잡성을 포함한 전체 우선순위 |

---

### 기술 개념 (쉬운 설명)

#### FRE (Frequency, Relational, contextual contribution)

**기술적 설명**: 세 가지 정규화된 어휘 지표의 가중 선형 조합: 코퍼스 빈도, 네트워크 중심성(PMI 기반 연결을 통해), 맥락적 중요성(TF-IDF 또는 유사 방법을 통해).

**쉬운 설명**: FRE는 "이 단어가 얼마나 유용한가?"에 답합니다. 빈도(Frequency)는 얼마나 자주 나타나는지 알려줍니다(흔한 단어가 더 유용합니다). 관계 밀도(Relational Density)는 얼마나 연결되어 있는지 알려줍니다(허브 단어는 관련 단어의 이해를 열어줍니다). 맥락적 기여(contextual contribution)는 얼마나 많은 의미를 전달하는지 알려줍니다(내용어가 기능어보다 이해에 더 중요합니다).

#### 전이 이득 (Transfer Gain)

**기술적 설명**: 특정 특성(음운론적, 형태론적, 어휘적)에 대한 L1-L2 언어 거리에서 도출된 계수로, 모국어 지식으로부터의 학습 비용 감소를 나타냅니다.

**쉬운 설명**: 스페인어를 사용하고 영어를 배우고 있다면, "hospital"은 거의 무료로 배울 수 있습니다(거의 동일합니다). 한국어를 사용한다면, 훨씬 더 어렵습니다(동족어 없음). 전이 이득은 각 특정 항목에 대해 모국어가 얼마나 도움이 되는지 포착합니다.

#### 긴급도 (Urgency)

**기술적 설명**: 간격 반복 원칙에 따라 항목이 예정된 복습 날짜에 접근하거나 초과할 때 증가하는 시간 기반 승수.

**쉬운 설명**: 긴급도는 "사용하지 않으면 잃는다" 요소입니다. 어제 복습 예정이었던 항목은 긴급합니다 - 곧 연습하지 않으면 잊어버리고 이전 학습을 낭비하게 됩니다. 아직 기한이 되지 않은 항목은 망각 위험이 없으므로 긴급도가 0입니다.

#### 큐 구축 (Queue Building)

**기술적 설명**: 최종 점수(우선순위 곱하기 긴급도 승수)로 어휘 항목을 정렬한 다음, 기한 항목 대 새 항목의 구성 가능한 비율로 균형 잡힌 세션을 추출하는 알고리즘.

**쉬운 설명**: 큐는 최적화된 학습 목록입니다. 가장 가치 있고 가장 긴급한 항목을 맨 위에 놓습니다. 그러나 복습(배운 것을 잊지 않기)과 새로운 학습(계속 진전하기)의 균형도 맞춥니다. 일반적인 세션은 70% 복습, 30% 새 항목일 수 있습니다.

---

### 사용 예시

#### 기본 우선순위 계산

```typescript
import { computePriority, DEFAULT_PRIORITY_WEIGHTS } from './priority';

const word = {
  id: 'medication-001',
  content: 'medication',
  type: 'lexical',
  frequency: 0.8,           // 의료 코퍼스에서 높은 빈도
  relationalDensity: 0.7,   // 많은 용어와 강한 연어 관계
  contextualContribution: 0.6, // 의미에 중요
  irtDifficulty: 0.5,       // 중간 난이도
};

const userState = {
  theta: 0.3,               // 평균보다 약간 높은 능력
  weights: DEFAULT_PRIORITY_WEIGHTS,
  l1Language: 'ko',         // 한국어 화자
};

const priority = computePriority(word, userState);
// ~2.1 반환 (높은 우선순위 - 가치 있고 배울 수 있음)
```

#### 학습 큐 구축

```typescript
import { buildLearningQueue, getSessionItems } from './priority';

const queue = buildLearningQueue(
  allVocabulary,
  userState,
  masteryMap,
  new Date()
);

// 세션용 20개 항목 가져오기 (70% 기한, 30% 새 항목)
const sessionItems = getSessionItems(queue, 20, 0.3);
```

#### 화용론을 포함한 강화된 우선순위

```typescript
import { computeEnhancedPriority, DEFAULT_ENHANCED_CONFIG } from './priority';

const config = {
  ...DEFAULT_ENHANCED_CONFIG,
  focusDomain: 'medical',
  pragmaticPenalty: true,
};

const result = computeEnhancedPriority(extendedWord, userState, config);
// result.priority: 최종 점수
// result.breakdown: 상세 비용 내역
```

---

### 변경 이력

#### 2026-01-06 - 문서 생성

- **변경 내용**: priority.ts에 대한 섀도우 문서 생성
- **이유**: 핵심 알고리즘은 시스템 이해를 위한 서술적 설명 필요
- **영향**: 개발자와 AI 에이전트가 우선순위 지정 로직을 이해할 수 있게 함

#### 역사적 구현 노트

- FRE 공식은 어휘 습득 연구에서 도출됨
- 전이 계수는 실제 L1-L2 거리 데이터 사용
- 긴급도 곡선은 FSRS 망각 곡선에 맞게 보정됨
- 강화된 우선순위는 초기 릴리스 후 화용론 통합 추가

---

## L1-L2 전이 계수 모듈 (L1-L2 Transfer Coefficient Module)

> **코드 위치**: `src/core/transfer.ts`

---

### 맥락 및 목적

이 모듈은 **교차언어적 영향(crosslinguistic influence)**의 과학을 구현합니다 - 학습자의 모국어(L1)가 새로운 언어(L2) 습득에 어떤 영향을 미치는지. 언어 학습이 백지 상태에서 시작하는 과정이 아니기 때문에 존재합니다; 학습자는 진전을 돕거나 방해할 수 있는 첫 번째 언어의 정신적 패턴, 소리, 구조를 가지고 있습니다.

**비즈니스 필요성**: LOGOS는 다양한 언어적 배경의 학습자들(영어를 배우는 스페인어 화자, 영어를 배우는 일본어 화자 등)을 대상으로 합니다. L1 전이 효과를 고려하지 않으면 시스템은 모든 학습자를 동일하게 취급하여 중요한 기회를 놓치게 됩니다:
- 긍정적 전이 활용 (L1 패턴이 L2 학습을 돕는 경우)
- 부정적 전이/간섭 예측 (L1 패턴이 오류를 유발하는 경우)
- 어휘와 문법에 대한 난이도 추정 개인화

**사용 시점**:
- 학습 항목의 "비용" 구성요소 조정을 위한 우선순위 계산 중
- 동족어 관계에 기반한 어휘의 IRT 난이도 보정 시
- 발음 훈련을 위한 음운론적 도전 예측 시
- 의료 도메인 보너스 적용 시 (로망스어 화자는 라틴어 기원 의료 용어에서 이점)

---

### 미시적 규모: 직접 관계

#### 의존성 (이 모듈이 필요로 하는 것)

이 모듈은 의도적으로 코드베이스 내에서 **의존성이 없습니다** - 다른 LOGOS 모듈로부터 임포트 없이 순수 함수를 포함합니다. 이 설계 선택은 다음을 보장합니다:
- 전이 계산이 독립적으로 테스트 가능
- 다른 핵심 모듈과의 순환 의존성 문제 없음
- 전이 로직이 다른 곳에서 사용되어야 할 경우 쉬운 이식성

**외부 데이터**: 모듈에 언어학 연구 데이터가 직접 포함됨:
- `LANGUAGE_FAMILIES`: ISO 639-1 언어 코드를 계통 분류에 매핑
- `ENGLISH_TRANSFER_COEFFICIENTS`: L2로서 영어에 대한 경험적 전이 값
- `MEDICAL_DOMAIN_TRANSFER`: 도메인별 동족어 보너스

#### 종속 모듈 (이 모듈을 필요로 하는 것)

- `src/core/priority.ts`: Priority = FRE / Cost 공식에서 "비용" 분모를 줄이기 위해 `calculateTransferGain()` 사용. 전이 이득이 높으면 학습 비용이 감소하여 우선순위 상승.

- `src/main/services/state-priority.service.ts`: 학습 큐 구축 시 전이 효과 통합, 긍정적 전이가 있는 항목이 적절히 표면화되도록 보장.

- `src/main/services/task-generation.service.ts`: 발음 중심 연습의 과제 난이도 조정을 위해 `getPhonologicalDifficultyBonus()` 사용.

- `src/core/content/content-generator.ts` (잠재적): 힌트 생성 및 설명 품질에 정보를 제공하기 위해 `isCognate()` 사용 가능.

#### 데이터 흐름

```
사용자 프로필 (L1 언어 코드)
    |
    v
getLanguageFamily(l1) --> LanguageFamily 분류
    |
    v
getTransferCoefficients(l1, l2) --> TransferCoefficients 객체
    |                                  (6개 언어학적 차원)
    v
calculateTransferGain(l1, l2, objectType, domain)
    |
    v
우선순위 계산 (비용 감소) --> 학습 큐 순서
```

---

### 거시적 규모: 시스템 통합

#### 아키텍처 레이어

이 모듈은 LOGOS의 3계층 아키텍처에서 **핵심 알고리즘 레이어**에 위치합니다:

```
레이어 1: 렌더러 (React UI)
    |
    v
레이어 2: 메인 프로세스 (IPC 핸들러, 서비스)
    |
    v
레이어 3: 핵심 알고리즘 <-- 여기입니다 (src/core/transfer.ts)
    |
    v
레이어 4: 데이터베이스 (Prisma/SQLite)
```

핵심 레이어 내에서, transfer.ts는 **우선순위 시스템** 서브그래프에 연결됩니다:

```
[FRE 지표] ----+
              |
              v
          priority.ts ---> 학습 큐
              ^
              |
[전이] -------+  (비용 요소 조정)
```

#### 전체적 영향

전이 모듈은 언어적 배경에 기반한 **개인화된 학습 경로**를 가능하게 합니다. 이것은 일반적인 언어 학습 앱과 비교하여 LOGOS를 차별화하는 기능 중 하나입니다.

**이 모듈 없이는** 시스템이:
- 영어를 배울 때 스페인어 화자와 일본어 화자를 동일하게 취급
- 로망스어 화자를 위한 동족어 우선순위 지정 기회 놓침
- 통사적 간섭 경고 실패 (SOV vs SVO 어순 충돌)
- 긍정적 전이가 적용되는 항목의 난이도 과대평가
- 부정적 전이가 지속적 오류를 유발하는 항목의 난이도 과소평가

**시스템 의존성**:
- **우선순위 계산**: `Priority = FRE / Cost`의 비용 요소가 전이 이득을 직접 포함
- **IRT 난이도 조정**: `calculateTransferAdjustedDifficulty()`가 항목 난이도 매개변수 수정
- **의료 도메인 전문화**: 로망스어 화자는 라틴어 기원 의료 용어에 동족어 보너스를 받아, CELBAN(캐나다 간호 자격증) 사용 사례 직접 지원

#### 중요 경로 분석

**중요도 수준**: 중상

이 모듈은 기본 앱 기능의 중요 경로에 있지 않지만(LOGOS는 이것 없이도 작동함), 다음에 필수적입니다:
- 개인화된 학습 효율성 (최적 항목 순서에 30-50% 예상 영향)
- 의료 도메인 효과성 (로망스어 화자에게 40% 동족어 보너스)
- 다양한 학습자 집단에 대한 정확한 난이도 예측

**실패 모드**: 이 모듈이 실패하거나 잘못된 값을 반환하면:
- 학습 우선순위가 차선이지만 치명적이지는 않음
- 난이도 추정이 덜 정확해짐
- 앱은 작동하지만 학습 효율성 감소

---

### 기술 개념 (쉬운 설명)

#### 전이 계수 (Transfer Coefficient)

**기술적 설명**: 모국어 패턴이 대상 언어 학습으로 전이되는 정도를 나타내는 -1에서 +1 사이의 수치 값. 양수 값은 촉진적 전이를, 음수 값은 간섭을 나타냅니다.

**쉬운 설명**: 새로운 나라에서 운전을 배우는 것처럼 생각하세요. 영국(좌측 통행)에서 배우고 미국(우측 통행)으로 이사하면 "부정적 전이"가 있습니다 - 옛 습관이 당신에게 불리하게 작용합니다. 캐나다에서 배우고 미국으로 이사하면 "긍정적 전이"가 있습니다 - 같은 쪽 도로, 쉬운 적응. 전이 계수는 옛 언어 습관이 새 언어에 얼마나 도움이 되거나 방해가 되는지 측정합니다.

**사용 이유**: 어떤 어휘, 문법 패턴, 소리가 특정 학습자 집단에게 쉽거나 어려울지 예측하여, 더 스마트한 항목 우선순위 지정을 가능하게 합니다.

#### 언어 계통 분류 (Language Family Classification)

**기술적 설명**: 공통 조상으로부터의 역사적 하강에 기반한 언어의 분류학적 그룹화 (예: 게르만어, 로망스어, 슬라브어, 중국-티베트어).

**쉬운 설명**: 언어는 생물학적 가족과 같습니다 - 영어, 독일어, 네덜란드어는 같은 조상(원시 게르만어)에서 내려온 언어학적 "형제"입니다. 스페인어, 프랑스어, 이탈리아어는 또 다른 형제 그룹입니다(라틴어에서 온 로망스어). 언어가 어떤 "가족"에 속하는지 알면 다른 언어와 패턴이 얼마나 유사한지 예측하는 데 도움이 됩니다.

**사용 이유**: 같은 계통의 언어는 어휘 어근, 문법 구조, 음성 체계를 공유합니다. 이를 통해 특정 언어 쌍에 대한 구체적인 연구 데이터가 없어도 전이 효과를 추정할 수 있습니다.

#### 동족어 (Cognate)

**기술적 설명**: 공통 어원을 공유하고 종종 유사한 형태를 가진 두 언어의 단어, 예: 영어 "telephone"과 스페인어 "telefono".

**쉬운 설명**: 동족어는 "무료 어휘"입니다 - 같은 역사적 어근을 공유하기 때문에 언어 간에 비슷하게 보이거나 들리는 단어. 영어를 배우는 스페인어 화자는 "hospital", "doctor", "family" 같은 수백 개의 단어를 이미 알고 있습니다. 두 언어에서 거의 동일하기 때문입니다.

**사용 이유**: `isCognate()` 함수는 학습자가 이미 인식할 수 있는 어휘를 식별하여 효과적인 학습 비용을 줄이고, 시스템이 동족어보다 진정으로 새로운 어휘를 우선시할 수 있게 합니다.

#### 음운론적 전이 (Phonological Transfer)

**기술적 설명**: 대상 언어 발음에 대한 모국어 음소 목록과 음소배열론적 제약의 영향.

**쉬운 설명**: 입은 모국어에서 특정 소리를 내도록 훈련되어 있습니다. 일본어에는 "L" 대 "R" 구별이 없어서 일본어 화자는 종종 영어에서 이것에 어려움을 겪습니다. 독일어 화자는 자신의 언어에 "ch" 소리가 있기 때문에 잘합니다. 음운론적 전이는 모국어가 사용하는 소리에 따라 어떤 소리가 쉽거나 어려울지 포착합니다.

**사용 이유**: 발음 과제의 난이도 점수를 조정하고 일반적인 발음 오류 패턴을 예측합니다.

#### 통사적 전이 (Syntactic Transfer)

**기술적 설명**: 대상 언어 생성에 대한 모국어 어순(예: 주어-동사-목적어 vs 주어-목적어-동사)과 문법 구조의 영향.

**쉬운 설명**: 언어는 단어를 다른 순서로 배치합니다. 영어는 주어-동사-목적어("I eat pizza")를 사용하고, 일본어는 주어-목적어-동사("I pizza eat")를 사용합니다. 새로운 언어를 배울 때, 뇌는 익숙한 어순을 사용하려고 하여 오류를 유발합니다. 부정적 통사 전이는 모국어 어순이 대상 언어와 충돌한다는 것을 의미합니다.

**사용 이유**: 영어 어순에 어려움을 겪을 학습자(일본어 및 한국어 화자 등)를 식별하고 그들을 위한 통사 패턴 연습의 우선순위를 높입니다.

#### 로짓 척도 (전이 수정자용) (Logit Scale for Transfer Modifier)

**기술적 설명**: 0이 평균 난이도를 나타내고, 양수 값은 더 어려운 항목을, 음수 값은 더 쉬운 항목을 나타내는 수학적 척도. 1 로짓 단위는 난이도의 의미 있는 차이를 나타냅니다.

**쉬운 설명**: 항목이 "어렵다" 또는 "쉽다"(주관적)고 말하는 대신, 0을 중심으로 하는 숫자 척도를 사용합니다. +1 값은 항목이 평균보다 1 "난이도 단위" 위라는 의미이고, -1은 1 단위 아래라는 의미입니다. 전이 수정자는 모국어가 얼마나 도움이 되거나 방해가 되는지에 따라 이 숫자를 위아래로 조정합니다.

**사용 이유**: 전이 수정자(`-transfer * 0.5` 로짓으로 계산)는 IRT 난이도 매개변수를 직접 조정하여, LOGOS의 심리측정적 기반과 통합됩니다.

#### 의료 도메인 전이 보너스 (Medical Domain Transfer Bonus)

**기술적 설명**: 의료 도메인의 어휘 항목에 적용되는 추가 긍정적 전이 계수로, 의료 영어에서 라틴어 및 그리스어 기원 용어의 높은 비율을 반영합니다.

**쉬운 설명**: 의료 어휘는 특별합니다. 많은 부분이 라틴어와 그리스어 어근에서 왔기 때문입니다. "cardiovascular", "hypertension", "pulmonary" 같은 단어는 영어, 스페인어, 프랑스어, 이탈리아어에서 거의 동일합니다. 로망스어 화자는 중국어나 아랍어 화자가 갖지 못한 의료 어휘에 대한 "선행 출발"을 얻습니다.

**사용 이유**: LOGOS는 특히 의료 전문가(CELBAN 자격증)를 대상으로 합니다. 이 보너스는 스페인어, 프랑스어, 포르투갈어, 이탈리아어 화자가 이미 인식하는 동족어에 시간을 낭비하지 않고 진정으로 낯선 의료 용어를 학습하도록 올바르게 우선순위가 지정되도록 합니다.

---

### 변경 이력

#### 2026-01-05 - 문서 생성
- **변경 내용**: 전이 모듈에 대한 초기 서술 문서
- **이유**: 섀도우 문서 시스템 구현
- **영향**: 모든 팀원이 L1-L2 전이 시스템을 이해할 수 있게 함

#### 초기 구현 - 모듈 생성
- **변경 내용**: 완전한 L1-L2 전이 계수 시스템 생성:
  - 40개 이상 언어에 대한 언어 계통 분류
  - 6차원 전이 계수 (음운론적, 정서법적, 형태론적, 어휘적, 통사적, 화용적)
  - SLA 연구 기반 영어별 전이 테이블
  - 로망스어 화자를 위한 의료 도메인 보너스
  - 동족어 감지 휴리스틱
- **이유**: 개인화된 학습은 모국어가 L2 습득에 어떤 영향을 미치는지 고려해야 함
- **영향**: 다른 언어적 배경의 학습자에 대한 차별화된 처리를 가능하게 하여, 전이가 도움이 되는 항목을 우선시하고 전이가 방해하는 항목을 발판으로 삼아 학습 효율성 향상

---

### 학술적 기반

이 모듈은 동료 심사 제2언어 습득(SLA) 연구에 기반합니다:

- **Jarvis, S. & Pavlenko, A. (2008)**: *Crosslinguistic Influence in Language and Cognition* - 언어학적 영역 간 전이 효과를 이해하기 위한 이론적 프레임워크
- **Ringbom, H. (2007)**: *Cross-linguistic Similarity in Foreign Language Learning* - 동족어 인식 및 전이 촉진에 관한 경험적 데이터
- **Odlin, T. (1989)**: *Language Transfer: Cross-linguistic influence in language learning* - 부정적 및 긍정적 전이 현상에 관한 기초 텍스트

`ENGLISH_TRANSFER_COEFFICIENTS`의 계수 값은 이 연구 문헌에서 종합된 경험적으로 도출된 추정치로, 계산 구현에 맞게 조정되었습니다.

---

## 우선순위-전이 통합 모듈 (Priority-Transfer Integration Module)

> **코드 위치**: `src/core/priority-transfer.ts`

---

### 맥락 및 목적

이 모듈은 언어 학습의 근본적인 질문에 답하기 위해 존재합니다: **"학습자가 모국어에서 이미 알고 있는 것을 바탕으로 단어가 얼마나 더 쉬워지거나 어려워질까?"**

우선순위-전이 모듈은 LOGOS의 두 가지 중요한 시스템을 연결합니다: **전이 계수 분석**(학습자의 모국어가 대상 언어와 얼마나 유사한지 측정)과 **학습 큐**(학습자가 다음에 무엇을 공부해야 하는지 결정). 이 다리 없이는 시스템이 모든 학습자를 동일하게 취급하여, 일본어 화자와 비교하여 영어 어휘를 학습할 때 독일어 화자가 가진 엄청난 이점을 무시하게 됩니다.

**비즈니스 필요성**: 각 학습자의 언어적 배경을 존중하는 개인화된 학습 경로. 의료 어휘를 배우는 스페인어 화자는 "cardio-"와 "-itis" 같은 라틴어 동족어를 활용할 수 있습니다 - 시스템은 이 이점을 인식하고 더 많은 연습이 필요한 진정으로 도전적인 어휘를 위해 이러한 "쉬운 승리"의 우선순위를 낮춰야 합니다.

**사용 시점**: 시스템이 학습자가 다음에 무엇을 공부해야 하는지 계산할 때마다. 이는 다음 중에 발생합니다:
- 세션 초기화 (학습 큐 구축)
- 응답 후 실시간 우선순위 재계산
- 새 어휘 도입 시 배치 처리
- 사용자 분석 (전이 강점/약점 표시)

---

### 미시적 규모: 직접 관계

#### 의존성 (이 모듈이 필요로 하는 것)

- `src/core/transfer.ts`: **getTransferCoefficients()** - L1과 L2 언어 계통 간의 원시 언어학적 유사성 점수(어휘적, 음운론적, 형태론적, 통사적, 화용적) 가져오기
- `src/core/transfer.ts`: **calculateTransferGain()** - 구성요소별 전이를 비용 계산을 위한 0-1 이득 점수로 변환
- `src/core/transfer.ts`: **getLanguageFamily()** - 모국어가 어떤 언어 계통(게르만어, 로망스어, 슬라브어 등)에 속하는지 결정
- `src/core/transfer.ts`: **TransferCoefficients, LanguageFamily** - 전이 데이터 구조의 타입 정의

#### 종속 모듈 (이 모듈을 필요로 하는 것)

현재 이 모듈은 다음에서 사용되도록 설계되었습니다:
- **세션 계획 서비스** - 모든 가용 항목을 순위화하기 위해 `calculateFullPriority()` 호출
- **큐 구축 로직** - 효율적인 대량 처리를 위해 `batchCalculateTransfer()` 사용
- **사용자 분석 대시보드** - 학습자에게 L1 이점을 보여주기 위해 `getTransferSummary()` 표시
- **IRT 매개변수 조정** - 난이도 추정에 `calculateCostWithTransfer()` 통합

#### 데이터 흐름

```
사용자 프로필 (모국어, 대상 언어)
    |
    v
TransferContext 생성 (언어 + 객체 유형 + 도메인)
    |
    v
getTransferCoefficients() L1-L2 유사성 매트릭스 가져오기
    |
    v
calculateTransferGain() 0-1 이득 점수로 변환
    |
    v
우선순위 조정: 높은 전이 = 낮은 우선순위 (이미 쉬움)
               낮은/부정적 전이 = 높은 우선순위 (주의 필요)
    |
    v
투명성을 위한 설명과 함께 최종 우선순위 반환
```

---

### 거시적 규모: 시스템 통합

#### 아키텍처 레이어

이 모듈은 LOGOS 아키텍처의 **알고리즘 핵심 레이어**에 위치합니다:

```
레이어 1: 렌더러 (React UI) - 사용자에게 학습 항목 표시
    |
레이어 2: IPC 브릿지 - UI와 메인 프로세스 간 통신
    |
레이어 3: 서비스 (state-priority.service.ts) - 우선순위 결정 조율
    |
레이어 4: 알고리즘 핵심 (이 모듈) - 순수 수학적 계산
    |
레이어 5: 데이터 레이어 (transfer.ts, priority.ts) - 원시 언어학 데이터
```

이 모듈은 부작용 없고, 데이터베이스 접근 없고, 상태 변경이 없는 **순수 계산**입니다. 입력을 받고 출력을 반환하여, 높은 테스트 가능성과 예측 가능성을 제공합니다.

#### 전체적 영향

우선순위-전이 모듈은 **언어학적으로 인식하는 적응형 학습**을 가능하게 합니다. 이것 없이 LOGOS는 두 가지 중요한 문제를 갖게 됩니다:

1. **낭비되는 학습 시간**: 학습자가 동족어(L1과 거의 동일한 단어)와 진정으로 어려운 어휘에 동일한 시간을 소비합니다. 프랑스어 화자는 "communication"을 배우는 데 20번의 반복이 필요하지 않습니다 - L1 앵커가 없는 단어에 그 반복이 필요합니다.

2. **놓친 간섭**: 일부 L1 패턴은 실제로 L2 학습을 *방해*합니다(부정적 전이). 일본어 SOV 어순은 영어 SVO 구조와 간섭을 일으킵니다. 이 모듈은 이러한 간섭이 발생하기 쉬운 항목을 감지하고 우선순위를 높입니다.

**시스템 전체 가치**:
- 진정한 학습 필요에 우선순위를 둠으로써 평균 숙달 시간 감소
- "쉬운" 항목에 대한 불필요한 작업을 줄여 학습자 동기 부여 향상
- 언어적 배경에 기반한 개인화된 추천 가능
- 설명 가능한 AI 제공 (모든 우선순위 결정에 사람이 읽을 수 있는 이유가 있음)

#### 중요 경로 분석

**중요도 수준**: 높음

이 모듈은 **선택적이지만 변혁적**입니다. 시스템은 이것 없이도 작동할 수 있지만(기본 우선순위 계산 사용), 학습 효율성이 크게 감소합니다:

- **실패 시**: 시스템이 언어 불가지론적 우선순위로 대체, 모든 학습자를 동일하게 취급
- **실패 모드**: 우아한 성능 저하 - 충돌 없음, 단지 덜 개인화된 학습
- **성능 영향**: 최소 - 항목당 O(1) 복잡도의 순수 수학 연산
- **복구**: 손상될 상태 없음, 올바른 L1/L2 구성으로 간단히 재시작

---

### 기술 개념 (쉬운 설명)

#### 전이 계수 (Transfer Coefficient)

**기술적 설명**: 특정 구성요소(어휘적, 음운론적, 통사적 등)에 대해 두 언어 간의 언어학적 유사성 정도를 나타내는 -1에서 +1 사이의 숫자 값.

**쉬운 설명**: 모국어와 배우고 있는 언어 사이의 "유사성 점수". +1은 거의 동일함을 의미하고(네덜란드어 어휘 → 영어), -1은 적극적으로 혼란스러움을 의미하고(일본어 어순 → 영어), 0은 중립(도움도 방해도 없음)을 의미합니다.

**사용 이유**: 언어 교사들이 직관적으로 알고 있는 것 - 특정 언어 쌍이 다른 것보다 배우기 쉽다는 것 - 을 정량화합니다.

#### 전이 이득 (Transfer Gain)

**기술적 설명**: 긍정적 L1-L2 전이로부터의 비용 감소를 나타내는 정규화된 0-1 값으로, -1에서 +1 전이 계수를 0-1 척도로 이동하여 계산.

**쉬운 설명**: 학습 난이도에 대한 "할인". 0.8의 이득은 모국어 배경 덕분에 이 항목이 80% 더 쉽다는 것을 의미하므로, 시스템은 반복 연습에 더 적은 시간을 소비할 수 있습니다.

**사용 이유**: 우선순위 공식은 양수가 필요하므로, 원시 유사성 점수를 "학습 비용 감소" 요소로 변환합니다.

#### 숙달 요소 g(m) - 역 U 곡선 (Mastery Factor g(m) - Inverted U-Curve)

**기술적 설명**: 중간 숙달(40-50%)에서 우선순위를 최대화하고, 초보자(기초 부족)와 고급 항목(이미 숙달됨) 모두에서 감소시키는 근접 발달 영역(Zone of Proximal Development) 이론 기반 함수.

**쉬운 설명**: 학습의 "골디락스 존". 막 시작한 항목(너무 어려움)과 숙달한 항목(너무 쉬움)은 낮은 우선순위를 받습니다. 학습 스윗 스팟(도전적이지만 달성 가능한)에 있는 항목은 가장 높은 우선순위를 받습니다.

**사용 이유**: 수십 년의 교육 심리학 연구에 기반 - 학습자는 현재 수준보다 약간 위의 자료에서 가장 빠르게 진전합니다.

#### 발판 격차 (Scaffolding Gap)

**기술적 설명**: 힌트/단서가 있을 때와 없을 때 학습자의 수행 차이로, 얼마나 많은 독립적 숙달이 달성되었는지 나타냅니다.

**쉬운 설명**: "힌트가 있으면 답할 수 있다"와 "냉정하게 답할 수 있다" 사이의 격차. 큰 격차는 보조 바퀴에 의존하고 있으며 더 많은 독립적 연습이 필요하다는 것을 의미합니다.

**사용 이유**: 패턴 매칭과 진정한 학습을 구별합니다. 항상 힌트가 필요한 학생은 자료를 진정으로 내재화하지 않았습니다.

#### 병목 부스트 (Bottleneck Boost)

**기술적 설명**: 선행 조건 차단자로 식별된 항목에 적용되는 우선순위 증가로, 학습 의존성 그래프의 위상 분석을 통해 계산.

**쉬운 설명**: 다른 학습을 열어주는 기초 항목에 대한 추가 우선순위. "run"을 모르면 "running"을 배울 수 없다면, "run"은 진전을 막고 있기 때문에 우선순위 부스트를 받습니다.

**사용 이유**: 학습자가 시간이 지남에 따라 복합되는 격차를 축적하지 않고 견고한 기초 위에 쌓도록 보장합니다.

#### 배치 처리 (Batch Processing)

**기술적 설명**: 중복 계수 조회를 줄이고 계산 효율성을 개선하기 위해 공유 컨텍스트로 단일 함수 호출에서 여러 항목을 처리.

**쉬운 설명**: 독일어-영어 유사성 테이블을 500번(단어당 한 번) 조회하는 대신, 한 번 조회하고 500개 단어 모두에 대량으로 적용합니다.

**사용 이유**: 성능 최적화. 수천 개의 어휘 항목을 처리할 때, 반복 조회를 피하면 시스템이 반응적으로 됩니다.

---

### 변경 이력

#### 2026-01-05 - 초기 문서화

- **변경 내용**: 우선순위-전이 모듈에 대한 서술 문서 생성
- **이유**: LOGOS 코드베이스에 대한 섀도우 맵 문서 시스템 구현
- **영향**: 미래 개발자가 전이 조정 우선순위 계산의 이유를 이해할 수 있게 함

---

### 공식 참조

이 모듈에서 구현된 완전한 우선순위 공식:

```
S_eff(w) = S_base(w) x g(m) x (1 + T_penalty) + Urgency + Bottleneck
```

여기서:
- **S_base(w)**: 기본 FRE 점수 (Frequency + Relational + Contextual 가중 합계)
- **g(m)**: 숙달 요소 (ZPD 이론의 역 U 곡선)
- **T_penalty**: 전이 조정 (L1-L2 유사성에 기반한 -0.25 ~ +0.25)
- **Urgency**: 간격 반복 스케줄링 압력
- **Bottleneck**: 의존성 그래프 우선순위 부스트

그리고 비용 공식:
```
Cost = BaseDifficulty - TransferGain + ExposureNeed
```

---

### 설계 결정

#### 높은 전이에 대해 우선순위를 낮추는 이유

반직관적인 통찰: 배우기 *더 쉬운* 항목은 *더 낮은* 우선순위를 가져야 합니다. 시스템은 완료 순서가 아닌 학습 효율성을 최적화합니다. 동족어는 학습 순서에 관계없이 빠르게 학습되므로, 시스템은 진정으로 반복이 필요한 항목에 주의를 집중합니다.

#### 전이 가중치로 0.25를 사용하는 이유

`TRANSFER_PRIORITY_WEIGHT = 0.25` 상수는 전이 효과를 눈에 띄게 하되 압도적이지 않게 선택되었습니다. 0.25에서, 최대 긍정적 전이는 우선순위를 12.5% 감소시키고, 최대 부정적 전이는 12.5% 증가시킵니다. 이는 전이를 여러 요소 중 하나로 유지하며, 빈도와 긴급도를 주요 동인으로 존중합니다.

#### 순수 함수를 사용하는 이유

이 모듈의 모든 함수는 순수합니다(부작용 없음, 결정론적 출력). 이 설계는 다음을 가능하게 합니다:
- 모킹 없이 쉬운 유닛 테스트
- 경쟁 조건 없는 병렬 처리
- 예측 가능한 디버깅 (같은 입력은 항상 같은 출력을 생성)
- 상태 손상 없이 개발 중 핫 리로딩

---

*이 문서의 원본: `docs/narrative/src/core/priority.md`, `transfer.md`, `priority-transfer.md`*
