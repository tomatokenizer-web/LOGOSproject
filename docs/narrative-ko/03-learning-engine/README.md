# 학습 엔진 (Learning Engine)

> **최종 업데이트**: 2026-01-07
> **상태**: Active
> **이전**: [언어학적 분석](../02-linguistic/README.md) | **다음**: [콘텐츠 생성](../04-content/README.md)

---

## 개요

학습 엔진은 LOGOS의 핵심 두뇌입니다. 이 시스템은 단순히 "다음에 무엇을 공부할까?"라는 질문에 답하는 것이 아니라, **왜 그것을 공부해야 하는지**, **얼마나 어려울지**, **현재 학습자에게 가장 효과적인 접근법이 무엇인지**를 종합적으로 판단합니다.

학습 엔진은 네 가지 핵심 하위 시스템으로 구성됩니다:

| 하위 시스템 | 역할 | 핵심 질문 |
|-------------|------|-----------|
| **우선순위 계산** | 학습 순서 결정 | "다음에 무엇을 배워야 하는가?" |
| **L1-L2 전이** | 모국어 영향 분석 | "모국어가 학습을 돕는가, 방해하는가?" |
| **병목 탐지** | 근본 원인 진단 | "실제로 무엇이 나를 막고 있는가?" |
| **PMI 분석** | 단어 연관성 측정 | "어떤 단어들이 함께 속하는가?" |

---

## 1. 우선순위 계산 시스템 (Priority Calculation)

### 핵심 개념: FRE 프레임워크

우선순위 계산의 핵심은 **FRE 공식**입니다:

```
우선순위 = (w_F x F + w_R x R + w_E x E) / 비용
```

#### F: 빈도 (Frequency) - 기초

고빈도 단어는 이해로 가는 가장 빠른 경로를 제공합니다. 상위 2,000개 단어가 텍스트의 약 80%를 커버합니다.

**쉬운 설명**: "the", "is", "have"는 거의 모든 문장에 나타납니다. 이것들을 먼저 배우면 즉시 이해가 향상됩니다.

#### R: 관계 밀도 (Relational Density) - 네트워크 효과

"take"와 같은 허브 단어는 수십 개의 구에 연결됩니다. 이것을 배우면 어휘 "힘의 승수"가 생깁니다.

**쉬운 설명**: "take"를 알면 "take off", "take over", "take in", "take up" 등을 더 쉽게 배울 수 있습니다.

#### E: 맥락적 기여 (Contextual Contribution) - 의미 제공자

무거운 의미 부담을 지는 단어. 고급 학습자는 정밀 어휘가 필요합니다.

**쉬운 설명**: "nevertheless"는 드물지만 학술 텍스트에서 중요한 논리적 관계를 표시합니다.

### 비용 계산

```
비용 = 기본난이도 - 전이이득 + 노출필요
```

- **기본난이도**: IRT에서 도출된 항목 고유 난이도
- **전이이득**: L1 동족어 이점 (모국어와 유사한 단어는 비용 감소)
- **노출필요**: 학습자 능력과 항목 난이도 간의 격차

### 긴급도: 간격반복 레이어

```
최종점수 = 우선순위 x (1 + 긴급도)
```

긴급도 규칙:
- 아직 예정 아님: 0
- 오늘 예정: 1
- N일 지연됨: min(3, 1 + N x 0.5)
- 새 항목: 1.5

**쉬운 설명**: 긴급도는 복습해야 할 항목을 대기열의 위로 밀어올립니다. 많이 지연될수록 더 긴급합니다. 이는 "사용하지 않으면 잃는다" 원칙을 구현합니다.

### 수준별 가중치 조정

학습자의 수준에 따라 FRE 가중치가 자동으로 조정됩니다:

| 수준 | Theta | F | R | E |
|------|-------|---|---|---|
| 초급 | < -1 | 0.5 | 0.25 | 0.25 |
| 중급 | -1 ~ +1 | 0.4 | 0.3 | 0.3 |
| 고급 | > +1 | 0.3 | 0.3 | 0.4 |

**이론적 근거**:
- **초급자**는 빈도를 우선시 - 핵심 어휘가 먼저
- **고급자**는 맥락적 기여를 우선시 - 정밀성과 뉘앙스 필요

### 세션 균형

기본 구성: 70% 예정 항목, 30% 새 항목

다음을 방지:
- 모두 복습 (진전 없음)
- 모두 새 항목 (기억 유지 없음)

---

## 2. L1-L2 전이 시스템 (Transfer System)

### 핵심 개념: 교차언어적 영향

언어 학습은 백지 상태에서 시작하지 않습니다. 학습자의 모국어(L1)는 새로운 언어(L2) 습득에 지대한 영향을 미칩니다.

**쉬운 설명**: 새로운 나라에서 운전을 배우는 것과 같습니다. 영국(좌측 통행)에서 배우고 미국(우측 통행)으로 이사하면 "부정적 전이"가 있습니다 - 오래된 습관이 당신에게 불리하게 작용합니다. 캐나다에서 배우고 미국으로 이사하면 "긍정적 전이"가 있습니다 - 같은 쪽 도로, 쉬운 조정.

### 전이 계수의 6가지 차원

| 차원 | 설명 | 예시 |
|------|------|------|
| **음운적** | 소리 체계 유사성 | 일본어 화자의 L/R 구별 어려움 |
| **철자적** | 문자 체계 유사성 | 로마자 사용 언어 간 이점 |
| **형태론적** | 단어 형태 유사성 | 어미 변화 패턴 |
| **어휘적** | 동족어 비율 | 스페인어-영어 간 많은 동족어 |
| **통사적** | 문장 구조 유사성 | SOV vs SVO 어순 |
| **화용론적** | 사용 규칙 유사성 | 공손함 표현 패턴 |

### 전이 이득과 비용 조정

전이 이득이 높으면 → 학습 비용 감소 → 우선순위 상승 (단, 직접 부스트가 아닌 비용 감소)

**쉬운 설명**: 스페인어 화자가 영어의 "hospital"을 배우는 것은 거의 무료입니다 (동족어). 그러나 이것이 고빈도 비동족어보다 더 중요하다는 것을 의미하지는 않습니다. 시스템은 "배우기 쉽다"와 "배울 가치가 있다"를 구별합니다.

### 의료 도메인 전이 보너스

LOGOS는 특히 의료 전문가(CELBAN 인증)를 대상으로 합니다. 의료 어휘는 많은 부분이 라틴어와 그리스어 어근에서 왔기 때문에:

- "cardiovascular", "hypertension", "pulmonary" 같은 단어들은 영어, 스페인어, 프랑스어, 이탈리아어에서 거의 동일
- 로망스어 화자들은 의료 어휘에 대해 최대 40% 동족어 보너스

---

## 3. 병목 탐지 시스템 (Bottleneck Detection)

### 핵심 개념: 계단식 오류 모델

병목 탐지는 증상이 아닌 근본 원인을 찾습니다.

**예시**: Maria가 어휘에서 85%, 절차 동사에서 30%의 성과를 보입니다. 이것은:
- 어휘 문제인가? (단어를 모름)
- 형태론 문제인가? (동사 형태를 처리할 수 없음)
- 통사 문제인가? (문장 구조에 어려움)

### 계단식 전파 구조

오류는 언어적 계층구조를 통해 전파됩니다:

```
PHON --> MORPH --> LEX --> SYNT --> PRAG
(음운)   (형태)    (어휘)  (통사)   (화용)
```

**핵심 통찰**: 형태론 문제는 어휘, 통사, 화용 과제에서 오류로 나타납니다. 순진한 접근법은 높은 LEX 오류를 찾고 어휘 훈련을 처방합니다. 계단식을 인식하는 접근법은 MORPH로 추적합니다.

### 병목 분석 알고리즘

```
계단식 순서의 각 구성요소에 대해 (PHON -> PRAG):
    오류율 >= 임계값 (30%)이면:
        하류 구성요소도 상승함 (>= 20%)이면:
            -> 이것이 근본 원인
            -> 하류 오류는 계단식 효과
```

### 실행 가능한 권장사항

시스템이 생성하는 권장사항:

1. **초점 진술**: "형태론에 집중 (46% 오류율)"
2. **계단식 이점**: "이를 개선하면 어휘, 문법에도 도움이 됩니다"
3. **구체적 패턴**: "특히 연습: -ed 어미"
4. **추세 피드백**: "(이미 개선 중 - 계속하세요!)"

### 시각화: 계단식 오류 전파

```
오류율
     |
100% |
     |
 50% |  ****        병목 (MORPH)
     |  *  *
 30% |  *  *  ****  계단식 효과 (LEX)
     |  *  *  *  *
 20% |  *  *  *  *  ****  (SYNT)
     |  *  *  *  *  *  *
 10% |  *  *  *  *  *  *
     +---------------------------
       PHON MORPH LEX SYNT PRAG

진짜 원인: MORPH (형태론)
표면 증상: LEX, SYNT 오류
```

---

## 4. PMI 분석 시스템 (Pointwise Mutual Information)

### 핵심 개념: 통계적 연관

PMI는 "어떤 단어들이 함께 속하는가?"에 답합니다.

**쉬운 설명**: 원어민은 "결정을 하다(make a decision)"라고 하지 "결정을 하다(do a decision)"라고 하지 않고, "진한 커피(strong coffee)"라고 하지 "강한 커피(powerful coffee)"라고 하지 않습니다. 이러한 조합은 언어에서 높은 상호정보량을 가지기 때문에 자연스럽게 느껴집니다.

### PMI 공식

```
PMI(w1, w2) = log2[ P(w1, w2) / (P(w1) x P(w2)) ]
```

해석:
- **PMI > 0**: 단어가 우연보다 더 자주 함께 나타남 (끌림)
- **PMI = 0**: 단어가 우연이 예측하는 대로 정확히 함께 나타남 (관계 없음)
- **PMI < 0**: 단어가 우연보다 덜 함께 나타남 (서로 밀어냄)

### 정규화된 PMI (NPMI)

일반 PMI의 문제점: 희귀한 쌍이 희귀하다는 이유만으로 터무니없이 높은 PMI를 가질 수 있음.

NPMI는 모든 것을 -1에서 +1 범위로 정규화:
- +1 = 완벽한 연관 (항상 함께 나타남)
- 0 = 독립 (관계 없음)
- -1 = 완벽한 배제 (절대 함께 나타나지 않음)

### PMI-IRT 브리지: 난이도 변환

PMI는 통계적 패턴을 심리측정 매개변수로 변환합니다:

```
높은 PMI (예: +8) --> 낮은 난이도 (예: -1.5)
     강한 연어 = 회상하기 더 쉬움

낮은 PMI (예: +1)  --> 높은 난이도 (예: +1.5)
     약한 연관 = 회상하기 더 어려움
```

**왜 이것이 작동하는가**: 기억 연구에 따르면 강하게 연관된 단어 쌍은 서로를 점화(prime)합니다. "strong"을 들으면 "coffee"라는 단어가 이미 뇌에서 부분적으로 활성화됩니다.

### FRE 우선순위와의 통합

PMI는 FRE 공식의 **R (관계 밀도)** 구성요소를 직접 공급합니다:

- 가지고 있는 유의미한 연어의 수
- 그 연어가 얼마나 강한지 (평균 PMI)
- 그 연어가 얼마나 다양한지 (도메인 전반의 분포)

높은-R 단어는 어휘 네트워크에서 "허브" 단어입니다. 이것들을 아는 것이 많은 맥락의 이해를 열어주기 때문에 배우기에 가치가 있습니다.

---

## 응답 타이밍: 품질 신호

### 핵심 개념: 속도는 지식의 질을 드러낸다

10초가 걸리는 정답은 800밀리초가 걸리는 정답과 매우 다른 지식을 나타냅니다.

| 응답 유형 | 의미 | 시스템 반응 |
|-----------|------|-------------|
| **자동적 지식** | 빠르고 유창한 응답 | 높은 FSRS 등급 |
| **노력을 들인 회상** | 느리지만 정확한 응답 | 중간 FSRS 등급 |
| **운 좋은 추측** | 의심스러울 정도로 빠름 | 추측으로 표시 |
| **게이밍 행동** | 로봇 같은 패턴 | 부정행위 감지 |

### 과제 범주별 임계값

다른 인지 과정은 근본적으로 다른 시간 프로필을 가집니다:

- **인식 과제**: 군중 속에서 친구 얼굴 알아보기 (빠름)
- **회상 과제**: 친구를 볼 때 이름 기억하기 (보통)
- **산출 과제**: 친구를 소개하기 (느림)

### 자동화 탐지

진정한 언어 유창성은 생각할 필요가 없다는 것을 의미합니다. "the cat sat on the"를 읽을 때 다음 단어가 무엇일지 즉시 알 수 있습니다.

자동화 임계값:
- 인식: 1000ms 미만
- 회상: 2000ms 미만
- 산출: 4000ms 미만

---

## 학술적 기반

### 우선순위 계산
- Anderson, J.R. (1982): 인지 기술 습득
- DeKeyser, R.M. (2007): 기술 습득 이론

### L1-L2 전이
- Jarvis, S. & Pavlenko, A. (2008): *Crosslinguistic Influence in Language and Cognition*
- Ringbom, H. (2007): *Cross-linguistic Similarity in Foreign Language Learning*
- Odlin, T. (1989): *Language Transfer*

### PMI 분석
- Church, K. & Hanks, P. (1990): Word association norms, mutual information, and lexicography

### 응답 타이밍
- Yap, M.J. & Balota, D.A. (2015): Visual word recognition
- Segalowitz, N. & Hulstijn, J. (2005): Automaticity in bilingualism

---

## 코드 위치

| 모듈 | 파일 경로 |
|------|-----------|
| 우선순위 계산 | `src/core/priority.ts` |
| L1-L2 전이 | `src/core/transfer.ts` |
| 병목 탐지 | `src/core/bottleneck.ts` |
| PMI 분석 | `src/core/pmi.ts` |
| 응답 타이밍 | `src/core/response-timing.ts` |

---

## 통합 데이터 흐름

```
사용자 프로필 (L1, 능력)
    |
    v
[전이 계수 계산] --> 비용 조정
    |
    v
[FRE 지표 계산] --> 가치 점수
    |
    v
[PMI 분석] --> 관계 밀도 (R)
    |
    v
[우선순위 = 가치 / 비용]
    |
    v
[긴급도 레이어] --> FSRS 통합
    |
    v
[세션 항목 선택] --> 70% 복습 + 30% 새 항목
    |
    v
[응답 타이밍 분석] --> 품질 신호
    |
    v
[병목 탐지] --> 근본 원인 진단
    |
    v
[다음 세션 우선순위 업데이트]
```

---

## 변경 이력

### 2026-01-07 - 통합 문서 생성
- **변경 사항**: 학습 엔진 관련 8개 문서를 하나의 통합 문서로 병합
- **이유**: 관련 개념들 간의 연결성 명확화 및 중복 제거
- **영향**: 학습 엔진 전체 아키텍처에 대한 통합적 이해 제공

---

**이전**: [언어학적 분석](../02-linguistic/README.md) | **다음**: [콘텐츠 생성](../04-content/README.md)
