# LOGOS 섀도우 문서 통합본 (한국어)

> **최종 업데이트**: 2026-01-07
> **문서 구조**: 인과관계적 파생 위계구조
> **네비게이션**: 코드 의존성과 기능적 연관성 기반

---

# 📍 네비게이션 맵

이 문서는 코드 간의 **인과관계적 필요성**과 **기능적 연관성**에 따라 구성되어 있습니다.

```
[계층 0: 기초 타입] ─────────────────────────────────────────────────────┐
    └── types.ts (모든 타입 정의)                                          │
                                                                          │
[계층 1: 수학적 기초] ◄───────────────────────────────────────────────────┤
    └── quadrature.ts (수치 적분)                                          │
        └─► IRT EAP 추정에서 사용                                          │
                                                                          │
[계층 2: 심리측정 엔진] ◄─────────────────────────────────────────────────┤
    ├── irt.ts (능력 추정) ◄── quadrature                                  │
    │   └─► 학습자 theta 추정, 항목 선택                                    │
    │                                                                      │
    └── fsrs.ts (기억 스케줄링) ◄── types                                  │
        └─► 복습 간격 계산, 숙달 단계 결정                                   │
                                                                          │
[계층 3: 언어학적 분석] ◄─────────────────────────────────────────────────┤
    ├── pmi.ts (연어 분석) ◄── types                                       │
    │   └─► 단어 연관성, 난이도 추정                                        │
    │                                                                      │
    ├── semantic-network.ts (의미 관계) ◄── 독립                           │
    │   └─► 동의어, 반의어, 상위어, 연어                                    │
    │                                                                      │
    ├── morphology.ts (형태소 분석) ◄── types                              │
    │   └─► 단어 구조, 파생 관계                                            │
    │                                                                      │
    ├── g2p.ts (발음 규칙) ◄── types                                       │
    │   └─► 철자-발음 대응                                                  │
    │       └── g2p-irt.ts ◄── g2p, irt                                   │
    │           └─► 발음 난이도의 IRT 통합                                  │
    │                                                                      │
    ├── syntactic.ts (문법 복잡성) ◄── types                               │
    │   └─► 문장 구조 분석                                                  │
    │                                                                      │
    ├── transfer.ts (L1-L2 전이) ◄── types                                │
    │   └─► 모국어 영향 예측                                                │
    │                                                                      │
    └── pragmatics.ts (화용론) ◄── types                                   │
        └─► 사용역, 적절성 분석                                             │
                                                                          │
[계층 4: 학습 최적화] ◄───────────────────────────────────────────────────┤
    ├── priority.ts (학습 순서) ◄── irt, fsrs, pmi                        │
    │   └─► FRE 우선순위, 학습 대기열                                       │
    │                                                                      │
    ├── bottleneck.ts (오류 진단) ◄── types                               │
    │   └─► 계단식 오류 분석, 근본 원인 탐지                                 │
    │                                                                      │
    ├── task-matching.ts (과제 선택) ◄── irt, priority                    │
    │   └─► 학습자-과제 적합성                                              │
    │                                                                      │
    ├── response-timing.ts (유창성) ◄── types, fsrs                       │
    │   └─► 응답 시간 분석, 자동화 탐지                                     │
    │                                                                      │
    └── stage-thresholds.ts (숙달 기준) ◄── types                         │
        └─► 단계 전환 조건                                                  │
                                                                          │
[계층 5: 콘텐츠 시스템] ◄─────────────────────────────────────────────────┤
    └── content/ ◄── 계층 2, 3, 4                                         │
        ├── pedagogical-intent.ts (교육적 의도)                             │
        │   └─► 9가지 학습 목적 정의                                        │
        ├── content-spec.ts (콘텐츠 명세) ◄── pedagogical-intent          │
        │   └─► 생성 요청 계약                                              │
        ├── content-generator.ts (콘텐츠 생성) ◄── content-spec           │
        │   └─► 대체 체인 (캐시 → 템플릿 → AI)                              │
        └── content-validator.ts (품질 검증) ◄── content-spec             │
            └─► 다차원 검증 시스템                                          │
                                                                          │
[계층 6: 과제 시스템] ◄───────────────────────────────────────────────────┤
    └── tasks/ ◄── 계층 4, 5                                              │
        ├── traditional-task-types.ts (30개 과제 유형)                     │
        │   └─► 과제 분류체계, 메타데이터                                   │
        ├── task-constraint-solver.ts (객체 선택) ◄── task-types          │
        │   └─► 제약 조건 충족, 점수화                                      │
        └── distractor-generator.ts (오답 생성) ◄── task-types            │
            └─► 언어학적 전략 기반 오답지                                    │
                                                                          │
[계층 7: 상태/문법/사용역] ◄──────────────────────────────────────────────┤
    ├── grammar/ ◄── syntactic, content                                   │
    │   ├── grammar-sequence-optimizer.ts                                 │
    │   └── syntactic-construction.ts                                     │
    │                                                                      │
    ├── register/ ◄── pragmatics                                          │
    │   ├── register-calculator.ts                                        │
    │   └── register-profile.ts                                           │
    │                                                                      │
    └── state/ ◄── types, bottleneck                                      │
        ├── component-object-state.ts                                     │
        └── component-search-engine.ts                                    │
                                                                          │
[배럴 익스포트]                                                            │
    └── index.ts ◄── 모든 모듈                                            │
        └─► 통합 API 표면                                                   │
└──────────────────────────────────────────────────────────────────────────┘
```

---

# 목차 (인과관계 순서)

## 제1부: 기초 계층 (Foundation Layer)
- [1.1 타입 시스템 기초](#11-타입-시스템-기초-typests) — 모든 타입 정의
- [1.2 코어 모듈 배럴](#12-코어-모듈-배럴-indexts) — 통합 API 표면

## 제2부: 심리측정 엔진 (Psychometric Engine)
- [2.1 문항반응이론 (IRT)](#21-문항반응이론-irt-irtts) — 능력 추정의 심리측정적 기반
- [2.2 간격반복 스케줄러 (FSRS)](#22-간격반복-스케줄러-fsrs-fsrsts) — 기억 스케줄링
  - 파생: [2.2.1 응답 타이밍](#221-응답-타이밍-response-timingts) — FSRS 등급 결정에 사용

## 제3부: 언어학적 분석 엔진 (Linguistic Analysis Engine)
- [3.1 점별 상호정보량 (PMI)](#31-점별-상호정보량-pmi-pmits) — 연어 및 난이도 분석
- [3.2 의미 네트워크](#32-의미-네트워크-semantic-networkts) — 단어 관계
- [3.3 형태론 분석](#33-형태론-분석-morphologyts) — 단어 구조
- [3.4 자소-음소 변환 (G2P)](#34-자소-음소-변환-g2p-g2pts) — 발음 규칙
- [3.5 통사적 복잡성](#35-통사적-복잡성-syntacticsts) — 문법 복잡성
- [3.6 L1-L2 전이](#36-l1-l2-전이-transferts) — 모국어 영향
- [3.7 화용론](#37-화용론-pragmaticsts) — 사용역과 적절성

## 제4부: 학습 최적화 엔진 (Learning Optimization Engine)
- [4.1 학습 우선순위](#41-학습-우선순위-priorityts) — FRE 기반 순서 결정
  - 의존: IRT (theta), FSRS (긴급도), PMI (관계 밀도)
- [4.2 병목 탐지](#42-병목-탐지-bottleneckts) — 계단식 오류 분석
  - 파생: 상태 관리 모듈에서 활용

## 제5부: 콘텐츠 생성 시스템 (Content Generation System)
- [5.1 교육적 의도](#51-교육적-의도-pedagogical-intentts) — 9가지 학습 목적
- [5.2 콘텐츠 명세](#52-콘텐츠-명세-content-spects) — 생성 요청 계약
- [5.3 콘텐츠 생성기](#53-콘텐츠-생성기-content-generatorts) — 대체 체인 생성
- [5.4 콘텐츠 검증기](#54-콘텐츠-검증기-content-validatorts) — 품질 보증

## 제6부: 과제 시스템 (Task System)
- [6.1 전통적 과제 유형](#61-전통적-과제-유형-traditional-task-typests) — 30개 과제 분류체계
- [6.2 과제 제약 조건 솔버](#62-과제-제약-조건-솔버-task-constraint-solverts) — 객체 선택
- [6.3 오답지 생성기](#63-오답지-생성기-distractor-generatorts) — MCQ 오답 생성

## 제7부: 파생 모듈 (Derived Modules)
- [7.1 문법 최적화](#71-문법-최적화-grammar) — 문법 시퀀싱
- [7.2 사용역 계산](#72-사용역-계산-register) — 사용역 프로필
- [7.3 상태 관리](#73-상태-관리-state) — 구성요소별 상태

---

# 제1부: 기초 계층 (Foundation Layer)

이 계층은 다른 모든 모듈이 의존하는 **기초**입니다. 타입 정의와 통합 API를 제공합니다.

---

## 1.1 타입 시스템 기초 (types.ts)

> **코드 위치**: `src/core/types.ts`
> **의존성**: 없음 (기초 계층)
> **피의존**: 모든 core 모듈

### 존재 이유

이 파일은 전체 LOGOS 애플리케이션의 **타입 시스템 기초**입니다. 핵심 알고리즘에서 사용하는 모든 데이터 구조를 정의하여 코드베이스 전반에 걸쳐 공유 어휘를 확립합니다. 이 파일이 없으면 LOGOS의 나머지 부분은 통신할 수 없습니다.

### 아키텍처적 위치

```
+------------------------------------------------------------+
| 계층 3: 렌더러 (React 컴포넌트)                              |
+------------------------------------------------------------+
                            |
                            v
+------------------------------------------------------------+
| 계층 2: IPC 브릿지 (contracts.ts, *.ipc.ts)                |
+------------------------------------------------------------+
                            |
                            v
+------------------------------------------------------------+
| 계층 1: 서비스 & 메인 프로세스                               |
+------------------------------------------------------------+
                            |
                            v
+------------------------------------------------------------+
| 계층 0: 핵심 타입 (이 파일)                                  |  ◄── 기초
+------------------------------------------------------------+
```

### 타입 도메인 구성

#### 1. IRT 타입 (문항반응이론)

| 타입 | 쉬운 설명 |
|------|-----------|
| `IRTModel` | 사용할 통계 모델 ('1PL', '2PL', '3PL') |
| `ItemParameter` | 학습하기 얼마나 어려운지에 영향을 미치는 어휘 항목의 속성 |
| `ThetaEstimate` | 학습자가 얼마나 능숙한지의 측정 (신뢰 구간 포함) |

**핵심 통찰**: Theta(능력)와 난이도는 같은 척도에 있습니다. theta가 0이면 평균 능력; 난이도가 0이면 평균 난이도. theta가 난이도와 같을 때 학습자는 50% 성공 확률을 가집니다.

#### 2. PMI 타입 (점별 상호정보량)

| 타입 | 쉬운 설명 |
|------|-----------|
| `PMIResult` | 두 단어 간 측정된 연관 강도 |
| `PMIPair` | 분석할 두 단어 |
| `CorpusStatistics` | 텍스트 모음에서의 단어 빈도에 대한 배경 데이터 |

**핵심 통찰**: 높은 PMI는 단어들이 우연보다 더 자주 함께 나타남을 의미합니다. "커피"와 "컵"은 높은 PMI를 가집니다.

#### 3. FSRS 타입 (간격반복)

| 타입 | 쉬운 설명 |
|------|-----------|
| `FSRSCard` | 항목의 기억 상태 (얼마나 잘 배웠는지, 얼마나 안정적인지) |
| `FSRSRating` | 학습자의 자기 평가 (1=잊음, 4=쉬움) |
| `FSRSParameters` | 알고리즘 튜닝 설정 |
| `FSRSScheduleResult` | 다음에 언제 복습할지 |

**핵심 통찰**: 안정성(S)은 잊을 때까지의 일수입니다. 알고리즘은 90% 기억 유지를 목표로 복습 타이밍을 최적화합니다.

#### 4. 숙달 타입

| 타입 | 쉬운 설명 |
|------|-----------|
| `MasteryStage` | 현재 기술 수준 (0-4) |
| `MasteryState` | 항목에 대한 완전한 학습 기록 |
| `ScaffoldingGap` | 힌트 있을 때와 없을 때의 성능 차이 |

#### 5. 과제 타입

| 타입 | 쉬운 설명 |
|------|-----------|
| `TaskType` | 연습의 종류 (인식, 회상, 산출 등) |
| `TaskFormat` | 연습의 제시 방식 (MCQ, 빈칸 채우기, 자유 응답) |
| `TaskSpec` | 생성할 연습에 대한 요청 |
| `TaskContent` | 실제 연습 내용 (프롬프트, 답, 힌트) |

#### 6. 우선순위 타입 (FRE 지표)

| 타입 | 쉬운 설명 |
|------|-----------|
| `FREMetrics` | 어휘에 대한 3차원 중요도 점수 |
| `PriorityCalculation` | 하나의 항목에 대한 완전한 순위 데이터 |
| `PriorityWeights` | 계산에서 F, R, E의 균형을 맞추는 방법 |

**핵심 통찰**: FRE는 Frequency(빈도), Relational density(관계 밀도), Contextual contribution(맥락적 기여)를 의미합니다.

#### 7. 병목 타입

| 타입 | 쉬운 설명 |
|------|-----------|
| `ComponentType` | 다섯 가지 언어 수준 (PHON, MORPH, LEX, SYNT, PRAG) |
| `BottleneckEvidence` | 한 수준에서의 오류 패턴 |
| `BottleneckAnalysis` | 오류 원인 진단 |
| `CascadeAnalysis` | 한 수준의 오류가 상위 수준의 오류를 어떻게 유발하는지 |

**핵심 통찰**: 언어 수준은 계단식으로 형성됩니다: 음운 -> 형태 -> 어휘 -> 통사 -> 화용.

---

## 1.2 코어 모듈 배럴 (index.ts)

> **코드 위치**: `src/core/index.ts`
> **의존성**: 모든 core 모듈
> **피의존**: 서비스 계층, IPC 계층, 렌더러

### 존재 이유

이 파일은 LOGOS 학습 알고리즘의 **중앙 신경계 게이트웨이**입니다. 내부 파일 구성을 알 필요 없이 모든 핵심 계산 함수에 접근할 수 있는 단일 통합 진입점을 제공하는 배럴 익스포트 역할을 합니다.

### 순수 알고리즘 격리 원칙

LOGOS는 엄격한 **순수 알고리즘 격리** 원칙을 따릅니다:
- 외부 의존성 없음
- 데이터베이스 호출 없음
- 네트워크 요청 없음
- 부작용 없음

이 단일 파일을 통해 모든 접근을 채널링함으로써:
1. 순수 함수가 순수하게 유지됨 (테스트 가능, 예측 가능, 이식 가능)
2. 구현 세부사항이 숨겨짐 (소비자를 깨뜨리지 않고 리팩터링)
3. 순환 의존성이 불가능해짐 (명확한 의존성 방향)

### 데이터 흐름

```
외부 요청 (IPC/서비스)
         |
         v
    [index.ts] -----> 적절한 알고리즘 선택
         |
         +---> IRT 함수 (능력 추정)
         |           |
         |           v
         |      세타 추정값 반환
         |
         +---> FSRS 함수 (스케줄링)
         |           |
         |           v
         |      다음 복습 날짜 반환
         |
         +---> 우선순위 함수 (큐 정렬)
         |           |
         |           v
         |      정렬된 학습 큐 반환
         |
         +---> 병목 함수 (진단)
                     |
                     v
                개입 권장 사항 반환
```

### 12개 알고리즘 영역

| 영역 | 설명 | 파일 |
|------|------|------|
| IRT | 학습자 능력 모델링 | `irt.ts` |
| FSRS | 망각 곡선 예측, 복습 스케줄링 | `fsrs.ts` |
| PMI | 단어 연관성 측정 | `pmi.ts` |
| 우선순위 | 빈도, 관계, 맥락을 학습 순서로 결합 | `priority.ts` |
| 병목 | 오류 연쇄 추적 (PHON→MORPH→LEX→SYNT→PRAG) | `bottleneck.ts` |
| 형태론 | 단어를 의미 있는 단위로 분해 | `morphology.ts` |
| G2P | 철자를 발음에 매핑 | `g2p.ts` |
| 통사론 | 문장 복잡성 분석 | `syntactic.ts` |
| 응답 타이밍 | 응답 속도 패턴 분석 | `response-timing.ts` |
| 과제 매칭 | 학습자에 적합한 연습 유형 선택 | `task-matching.ts` |
| 전이 | L1으로부터의 긍정적/부정적 전이 예측 | `transfer.ts` |
| 단계 임계값 | 학습 단계 진행 기준 정의 | `stage-thresholds.ts` |

---

# 제2부: 심리측정 엔진 (Psychometric Engine)

이 계층은 **학습자 모델링**의 심리측정학적 기반을 제공합니다. IRT와 FSRS가 핵심입니다.

---

## 2.1 문항반응이론 (IRT) (irt.ts)

> **코드 위치**: `src/core/irt.ts`
> **의존성**: `types.ts`, `quadrature.ts`
> **피의존**: `priority.ts`, `task-matching.ts`, `g2p-irt.ts`, 서비스 계층

### 존재 이유

이 모듈은 **문항반응이론(Item Response Theory)**을 구현합니다 - LOGOS 적응형 학습 시스템의 심리측정학적 백본입니다. IRT를 통해 LOGOS는 학습자의 응답 패턴을 기반으로 얼마나 숙련되었는지 추정하고, 그 기술 수준에 대해 가장 많은 정보를 제공할 항목을 선택할 수 있습니다.

**비즈니스 필요성**: 언어 학습 플랫폼은 개별 학습자에게 적응해야 합니다. IRT 없이는 시스템이 모두에게 같은 항목을 제공하거나(기술 차이 무시) "정답률"과 같은 조잡한 지표에 의존합니다(항목 난이도를 고려하지 않음).

### 아키텍처적 위치

```
+-----------------------------------------------+
| 렌더러: SessionView, QuestionCard              |
+-----------------------------------------------+
                    |
                    v
+-----------------------------------------------+
| IPC: session.ipc.ts, learning.ipc.ts          |
+-----------------------------------------------+
                    |
                    v
+-----------------------------------------------+
| 서비스: scoring-update, task-generation        |
+-----------------------------------------------+
                    |
                    v
+-----------------------------------------------+
| 핵심: IRT 모듈 (이 파일)                        |  ◄── 현재 위치
| 순수 함수, I/O 없음, 부작용 없음                 |
+-----------------------------------------------+
```

### 세 가지 IRT 모델

#### 1PL (라쉬 모델)

```
P(정답) = 1 / (1 + e^(-(theta - b)))
```

**쉬운 설명**: 항목을 맞힐 확률은 그것이 당신의 기술(theta)보다 얼마나 더 어려운지(b)에만 의존합니다. 모든 항목은 기술 차이에 대해 동일하게 "민감한" 것으로 가정됩니다.

**사용처**: 변별도가 비교적 균일한 음운 항목.

#### 2PL 모델

```
P(정답) = 1 / (1 + e^(-a(theta - b)))
```

**쉬운 설명**: 1PL과 같지만 항목이 다른 "날카로움"(a)을 가질 수 있습니다. 높은-a 항목은 숙련된 학습자와 미숙련 학습자를 날카롭게 구별합니다; 낮은-a 항목은 기술과 관계없이 비슷한 결과를 줍니다.

**사용처**: 변별도가 다양한 어휘 및 통사 항목.

#### 3PL 모델

```
P(정답) = c + (1-c) / (1 + e^(-a(theta - b)))
```

**쉬운 설명**: 2PL과 같지만 추측을 고려합니다. 매우 낮은 기술의 사람도 때때로 올바르게 추측할 수 있습니다. 매개변수 c는 추측 하한입니다.

**사용처**: 추측이 가능한 화용 항목 및 선다형 문제.

### Theta 추정 방법

#### 최대우도추정 (MLE)

**하는 일**: 관찰된 응답 패턴을 가장 가능하게 만드는 theta 값을 찾습니다.

**장점**: 불편 추정치, 효율적
**단점**: 극단적 패턴에서 발산

**사용 시점**: 혼합된 결과를 가진 5개 이상의 응답이 있을 때.

#### 사후기대값 (EAP)

**하는 일**: 응답과 사전 신념이 주어졌을 때 theta의 사후 분포의 평균을 계산합니다.

**장점**: 항상 유한, 사전 지식 통합, 적은 응답에서 안정적
**단점**: 사전 쪽으로 편향

**사용 시점**: 항상, 특히 학습 초기나 극단적 패턴에서. LOGOS는 EAP를 기본으로 사용합니다.

### 항목 선택: Fisher 정보 최대화

**하는 일**: theta에 대한 불확실성을 가장 많이 줄일 항목을 선택합니다.

**작동 방식**:
1. 각 후보 항목에 대해 현재 theta에서 Fisher 정보 계산
2. Fisher Info = a² × P × (1-P)
3. 최대 정보를 가진 항목 선택

```
정보
     |
     |        *****
     |      **     **
     |    **         **
     |  **             **
     | *                 *
     |*                   *
     +-------------------------> Theta
        (theta = b일 때)
```

최대 정보는 theta가 난이도(b)와 같을 때 발생합니다.

### IRT가 가능하게 하는 것

| 기능 | IRT가 어떻게 가능하게 하는가 |
|------|----------------------------|
| 적응형 난이도 | 학습자 능력 수준에서 항목 선택 |
| 능력 추적 | Theta 추정치가 시간에 따른 진행을 보여줌 |
| 효율적 학습 | 최대 정보 항목이 낭비된 연습을 최소화 |
| 공정한 평가 | 점수가 항목 난이도를 고려 |
| 보정 | 새 항목이 학습자 응답으로부터 보정됨 |

---

## 2.2 간격반복 스케줄러 (FSRS) (fsrs.ts)

> **코드 위치**: `src/core/fsrs.ts`
> **의존성**: `types.ts`
> **피의존**: `priority.ts`, `response-timing.ts`, 서비스 계층

### 존재 이유

인간의 뇌는 망각 기계입니다. 새로운 것을 배운 후 24시간 내에 약 70%를 잊습니다. 일주일 내에 90%를 잊습니다. 하지만 기억 과학의 반직관적인 통찰이 있습니다: **거의 잊어버릴 뻔한 것을 성공적으로 회상하는 행위가 기억을 영구적으로 만듭니다.**

**FSRS 모듈은 언어 습득이 근본적으로 기억 문제이기 때문에 존재합니다.** 체계적인 기억 관리 없이는 학습자들이 막대한 시간을 낭비합니다:
- 이미 아는 것을 복습 (노력 낭비)
- 잊고 있는 것을 복습하지 않음 (지식 쇠퇴)

### FSRS 알고리즘

FSRS는 **두 변수 기억 모델**을 사용합니다:
- **안정성 (S)**: 기억이 90% 회상 확률로 쇠퇴할 때까지의 기간
- **난이도 (D)**: 이 학습자에게 이 항목이 본질적으로 얼마나 어려운지

핵심 공식: `검색가능성 = e^(-t/S)` 여기서 t = 경과 일수, S = 안정성.

### 숙달 단계와의 관계 (0-4)

| 단계 | 이름 | 기준 |
|------|------|------|
| 0 | 미지 | 한 번도 접하지 않음 |
| 1 | 인식 | cueAssistedAccuracy >= 0.5 |
| 2 | 회상 | cueFreeAccuracy >= 0.6 OR cueAssistedAccuracy >= 0.8 |
| 3 | 통제 | cueFreeAccuracy >= 0.75 AND stability > 7일 |
| 4 | 자동 | cueFreeAccuracy >= 0.9 AND stability > 30일 AND gap < 0.1 |

FSRS 매개변수는 안정성 요구사항을 통해 단계 전환에 직접적으로 정보를 제공합니다.

### FSRS 평점 시스템 (1-4)

| 평점 | 이름 | LOGOS 변환 |
|------|------|------------|
| 1 | Again | 오답 |
| 2 | Hard | 단서와 함께 정답 |
| 3 | Good | 정답, 단서 없이, 느림 (>5초) |
| 4 | Easy | 정답, 단서 없이, 빠름 (<=5초) |

### 망각 곡선 시각화

```
회상 확률
   1.0 |*
       |  *
   0.9 |    *       ← 목표 유지율 (90%)
       |      *
   0.7 |        *
       |          *
   0.5 |            *
       |              *
   0.3 |                *
       |                  *
   0.1 |                    *   *   *
       +----------------------------> 시간 (일)
           S         2S        3S

S = 안정성 (일)
복습은 R이 90%로 떨어질 때 스케줄됨
```

---

## 2.2.1 응답 타이밍 (response-timing.ts)

> **코드 위치**: `src/core/response-timing.ts`
> **의존성**: `types.ts`, `fsrs.ts` (평점 결정에 사용)
> **피의존**: `scoring-update.service.ts`

### 존재 이유 (FSRS와의 인과관계)

응답 타이밍 모듈은 **FSRS 평점 결정**에 필수적입니다. 원시 응답 시간을 의미 있는 학습 신호로 변환하여 FSRS가 적절한 복습 간격을 스케줄링할 수 있게 합니다.

```
사용자가 과제 완료
        |
        v
응답 시간 (ms) 캡처
        |
        v
analyzeResponseTime()이 응답 분류
        |
        v
calculateFSRSRatingWithTiming()이 FSRS 등급 생성 (1-4)  ← FSRS와 연결
        |
        v
FSRS 알고리즘이 등급을 사용하여 다음 복습 예약
```

### 응답 분류

| 분류 | 의미 | 시사점 |
|------|------|--------|
| 자동적 | 빠르고 정확 | 깊은 학습 → FSRS 4 (Easy) |
| 노력을 들인 회상 | 느리지만 정확 | 취약한 기억 → FSRS 3 (Good) |
| 운 좋은 추측 | 의심스럽게 빠름 | 실제 지식 아님 → 평점 하향 |
| 게이밍 행동 | 로봇 같은 패턴 | 부정행위 시도 → 무효화 |

### 과제 범주별 임계값

| 과제 범주 | 빠른 | 좋은 | 느린 |
|-----------|------|------|------|
| 인식 | < 800ms | 800-2000ms | > 2000ms |
| 회상 | < 1500ms | 1500-4000ms | > 4000ms |
| 산출 | < 3000ms | 3000-8000ms | > 8000ms |

### 자동화 탐지

단계 4 숙달 (자동화)은 학습자가 지식에 대한 빠르고 노력 없는 접근을 보여줄 때만 부여됩니다. 응답 타이밍은 이 "자동화" 상태를 검증하는 핵심 신호입니다.

---

# 제3부: 언어학적 분석 엔진 (Linguistic Analysis Engine)

이 계층은 언어 자체를 분석합니다. 단어 관계, 형태, 발음, 문법, 화용 등을 다룹니다.

---

## 3.1 점별 상호정보량 (PMI) (pmi.ts)

> **코드 위치**: `src/core/pmi.ts`
> **의존성**: `types.ts`
> **피의존**: `priority.ts` (R 구성요소), `pmi.service.ts`

### 존재 이유

PMI 모듈은 다음에 답합니다: **어떤 단어들이 "함께 속하는가"?**

언어 학습에서 연어(자연스럽게 함께 나타나는 단어)를 이해하는 것은 유창하게 들리는 것과 교과서처럼 들리는 것의 차이입니다. 원어민은 "결정을 하다(make a decision)"라고 하지 "결정을 하다(do a decision)"라고 하지 않습니다.

### PMI 공식

```
PMI(w1, w2) = log2[ P(w1, w2) / (P(w1) * P(w2)) ]
```

- **PMI > 0**: 단어가 우연보다 더 자주 함께 나타남. 끌림.
- **PMI = 0**: 단어가 우연이 예측하는 대로 정확히 함께 나타남.
- **PMI < 0**: 단어가 우연보다 덜 함께 나타남. 서로 밀어냄.

### PMI-IRT 브리지 (핵심 인과관계)

이것은 모듈에서 가장 중요한 아키텍처 결정입니다. `pmiToDifficulty()` 함수는 통계적 패턴을 심리측정 매개변수로 변환합니다:

```
높은 PMI (예: +8) --> 낮은 난이도 (예: -1.5)
     강한 연어 = 회상하기 더 쉬움

낮은 PMI (예: +1)  --> 높은 난이도 (예: +1.5)
     약한 연관 = 회상하기 더 어려움
```

### FRE 우선순위와의 통합

PMI는 FRE 우선순위 시스템의 **R (관계 밀도)**을 직접 공급합니다:

```
priority.ts 에서:
우선순위 = (w_F × F + w_R × R + w_E × E) / 비용
                        ↑
                    PMI에서 계산
```

---

## 3.2 의미 네트워크 (semantic-network.ts)

> **코드 위치**: `src/core/semantic-network.ts`
> **의존성**: 없음 (자체 완결형)
> **피의존**: `priority.ts`, `distractor-generator.ts`, `content-generator.ts`

### 존재 이유

이 모듈은 어휘 관계를 위한 **의미 네트워크 모델**을 구현합니다. 단어들이 서로 어떻게 연결되는지 이해할 수 있게 합니다 - 동의어, 반의어, 상위어(범주 관계), 그리고 연어를 통해서.

### 어휘 관계 유형

#### 동의어 (동일한 의미)

유사한 의미를 가지며 때때로 서로 대체할 수 있는 단어들.

**핵심 통찰**: 동의어는 완벽하게 대체 가능하지 않습니다. "Big"과 "large"는 동의어이지만, "big sister"는 "large sister"가 아닙니다.

#### 반의어 (반대 의미)

| 유형 | 설명 | 예시 |
|------|------|------|
| 등급적 | 척도상의 정도 | hot-cold |
| 상보적 | 중간 지점 없음 | dead-alive |
| 관계적 | 반대 역할 | buy-sell |

#### 상위어/하위어 (범주 관계)

상위어 = 더 일반적인 범주 ("animal"). 하위어 = 더 구체적인 유형 ("dog").

**핵심 통찰**: 이것은 상속을 만듭니다. "dog"를 알면, "animal"의 일부 속성을 이미 알고 있는 것입니다.

#### 연어 (단어 파트너십)

자연어에서 자주 함께 나타나는 단어들. "Make a decision"은 자연스럽지만; "do a decision"은 그렇지 않습니다.

### 파생 관계: 오답지 생성

`distractor-generator.ts`에서 의미적으로 관련된 오답을 생성할 때 이 모듈을 사용합니다:

```
정답: "dog"
의미적 관련 오답: "cat", "bird", "fish"  ← semantic-network에서 조회
```

---

## 3.3 형태론 분석 (morphology.ts)

> **코드 위치**: `src/core/morphology.ts`
> **의존성**: `types.ts`
> **피의존**: `bottleneck.ts` (MORPH 오류 분석), `task-generation`

### 존재 이유

형태론 분석 모듈은 단어를 의미 있는 최소 단위(형태소)로 분해합니다. 이를 통해 LOGOS는:
- 단어 형성 패턴을 식별
- 파생어 관계를 추적
- 형태론적 복잡성을 계산
- 학습자의 형태론적 지식을 평가

### 형태소 유형

| 유형 | 설명 | 예시 |
|------|------|------|
| 어근 (root) | 핵심 의미 | "walk" in "walking" |
| 접두사 (prefix) | 어근 앞에 붙음 | "un-" in "unhappy" |
| 접미사 (suffix) | 어근 뒤에 붙음 | "-ing" in "walking" |
| 굴절 (inflection) | 문법적 기능 | "-s" in "cats" |
| 파생 (derivation) | 새 단어 생성 | "-ness" in "happiness" |

### 병목 탐지와의 관계

MORPH 수준의 오류가 감지되면 형태론 분석 결과가 어떤 패턴에서 문제가 있는지 식별합니다:

```
bottleneck.ts 분석 결과:
  rootCause: MORPH
  patterns: ["-ed 어미 (7회)", "-ing 어미 (4회)"]
            ↑
      morphology.ts에서 분류
```

---

## 3.4 자소-음소 변환 (G2P) (g2p.ts)

> **코드 위치**: `src/core/g2p.ts`
> **의존성**: `types.ts`
> **피의존**: `g2p-irt.ts`, `bottleneck.ts` (PHON 오류 분석)

### 존재 이유

자소-음소 변환(Grapheme-to-Phoneme) 모듈은 철자와 발음 간의 관계를 분석합니다. 이는 언어 학습에서 핵심적인 도전 중 하나입니다.

### 변환 규칙 유형

| 투명도 | 설명 | 예시 |
|--------|------|------|
| 투명 | 일관된 1:1 대응 | "cat" → /kæt/ |
| 반투명 | 맥락 의존적 | "c" → /k/ or /s/ |
| 불투명 | 불규칙적 | "colonel" → /ˈkɜːrnəl/ |

### G2P-IRT 파생 모듈

`g2p-irt.ts`는 G2P 복잡성을 IRT 난이도 추정에 통합합니다:

```
g2p.ts
  └── 불규칙 발음 감지
          ↓
g2p-irt.ts
  └── IRT 난이도 조정
          ↓
irt.ts
  └── 조정된 난이도로 항목 선택
```

---

## 3.5 통사적 복잡성 (syntactic.ts)

> **코드 위치**: `src/core/syntactic.ts`
> **의존성**: `types.ts`
> **피의존**: `bottleneck.ts` (SYNT 오류 분석), `grammar/`

### 존재 이유

통사적 복잡성 모듈은 문법 구조의 난이도를 분석합니다. 학습자 수준에 맞는 적절한 문법 과제를 선택할 수 있게 합니다.

### 복잡성 요인

| 요인 | 설명 | 예시 |
|------|------|------|
| 절 깊이 | 내포된 절의 수 | "The man [who saw the dog [that bit the cat]]" |
| 이동 거리 | 요소 이동 거리 | Wh-이동, 주제화 |
| 의존성 길이 | 문법적 의존성 거리 | 주어-동사 일치 거리 |
| 비연속성 | 끊어진 구성요소 | 분리된 동사구 |

### grammar/ 모듈과의 관계

```
syntactic.ts
  └── 복잡성 분석 제공
          ↓
grammar/grammar-sequence-optimizer.ts
  └── 문법 시퀀싱에 복잡성 사용
```

---

## 3.6 L1-L2 전이 (transfer.ts)

> **코드 위치**: `src/core/transfer.ts`
> **의존성**: `types.ts`
> **피의존**: `priority.ts` (비용 계산), `task-generation`

### 존재 이유

전이 모듈은 학습자의 모국어(L1)가 목표 언어(L2) 학습에 미치는 영향을 예측합니다.

### 전이 유형

| 유형 | 효과 | 예시 |
|------|------|------|
| 긍정적 전이 | 학습 촉진 | 동족어 (cognate) |
| 부정적 전이 | 학습 방해 | 거짓 동족어 (false friend) |
| 중립적 | 영향 없음 | 완전히 새로운 개념 |

### 우선순위 계산에서의 역할

```
priority.ts의 비용 계산:
비용 = 기본난이도 - 전이이득 + 노출필요
                    ↑
              transfer.ts에서 계산
```

---

## 3.7 화용론 (pragmatics.ts)

> **코드 위치**: `src/core/pragmatics.ts`
> **의존성**: `types.ts`
> **피의존**: `bottleneck.ts` (PRAG 오류 분석), `register/`

### 존재 이유

화용론 모듈은 언어 사용의 맥락적 측면을 분석합니다: 사용역(register), 공손함, 적절성.

### 사용역 차원

| 차원 | 범위 | 예시 |
|------|------|------|
| 격식 수준 | formal - informal | "proceed" vs "go" |
| 기술성 | technical - lay | "hypertension" vs "high blood pressure" |
| 친밀도 | intimate - distant | "honey" vs "sir/madam" |

### register/ 모듈과의 관계

```
pragmatics.ts
  └── 화용적 특성 분석
          ↓
register/register-calculator.ts
  └── 사용역 점수 계산
          ↓
register/register-profile.ts
  └── 도메인별 프로필 적용
```

---

# 제4부: 학습 최적화 엔진 (Learning Optimization Engine)

이 계층은 이전 계층들의 결과를 **학습 최적화**에 통합합니다.

---

## 4.1 학습 우선순위 (priority.ts)

> **코드 위치**: `src/core/priority.ts`
> **의존성**: (암시적) `irt.ts`, `fsrs.ts`, `pmi.ts`, `transfer.ts`
> **피의존**: 서비스 계층, 세션 관리

### 존재 이유

우선순위 계산 모듈은 언어 학습에서 가장 근본적인 질문에 답합니다: **다음에 무엇을 배워야 하는가?**

### 핵심 공식

```
우선순위 = (w_F × F + w_R × R + w_E × E) / 비용
```

### 다른 모듈과의 통합 (인과관계)

```
                    pmi.ts
                      ↓ (R: 관계 밀도)
                      ↓
priority.ts ◄─────────┼─────────◄ transfer.ts (비용: 전이이득)
                      │
                      ↓ (긴급도)
                    fsrs.ts
```

### FRE 프레임워크

| 구성요소 | 이름 | 소스 | 설명 |
|----------|------|------|------|
| F | Frequency | 코퍼스 분석 | 얼마나 자주 나타나는지 |
| R | Relational Density | PMI 분석 | 다른 단어와 얼마나 연결되는지 |
| E | Contextual Contribution | 의미 분석 | 이해에 얼마나 기여하는지 |

### 긴급도: FSRS와의 통합

```
최종점수 = 우선순위 × (1 + 긴급도)
```

| 상태 | 긴급도 |
|------|--------|
| 아직 예정 아님 | 0 |
| 오늘 예정 | 1 |
| N일 지연됨 | min(3, 1 + N × 0.5) |
| 새 항목 | 1.5 |

### 수준별 가중치 조정

| 수준 | Theta | F | R | E |
|------|-------|---|---|---|
| 초급 | < -1 | 0.5 | 0.25 | 0.25 |
| 중급 | -1 ~ +1 | 0.4 | 0.3 | 0.3 |
| 고급 | > +1 | 0.3 | 0.3 | 0.4 |

**이론적 근거**:
- **초급자**는 빈도를 우선시 - 핵심 어휘가 먼저
- **고급자**는 맥락적 기여를 우선시 - 정밀성과 뉘앙스 필요

---

## 4.2 병목 탐지 (bottleneck.ts)

> **코드 위치**: `src/core/bottleneck.ts`
> **의존성**: `types.ts`
> **피의존**: `state/`, 에이전트 트리거, 분석 대시보드

### 존재 이유

병목 탐지 모듈은 다음에 답합니다: **"실제로 무엇이 나를 막고 있는가?"**

**병목 탐지기는 증상이 아닌 근본 원인을 찾습니다.**

### 계단식 모델

오류는 언어적 계층구조를 통해 전파됩니다:

```
PHON → MORPH → LEX → SYNT → PRAG
(음운) → (형태) → (어휘) → (통사) → (화용)
```

| 위치 | 구성요소 | 분석 소스 |
|------|----------|-----------|
| 1 | PHON (음운론) | g2p.ts |
| 2 | MORPH (형태론) | morphology.ts |
| 3 | LEX (어휘) | semantic-network.ts, pmi.ts |
| 4 | SYNT (통사론) | syntactic.ts |
| 5 | PRAG (화용론) | pragmatics.ts |

### 계단식 분석 알고리즘

```
계단식 순서의 각 구성요소에 대해 (PHON → PRAG):
    오류율 >= 임계값 (30%)이면:
        하류 구성요소도 상승함 (>= 20%)이면:
            → 이것이 근본 원인
            → 하류 오류는 계단식 효과
```

### 시각화: 계단식 오류 전파

```
오류율
     |
100% |
     |
 50% |  ****        병목 (MORPH)
     |  *  *
 30% |  *  *  ****  계단식 효과 (LEX)
     |  *  *  *  *
 20% |  *  *  *  *  ****  (SYNT)
     |  *  *  *  *  *  *
 10% |  *  *  *  *  *  *
     +---------------------------
       PHON MORPH LEX SYNT PRAG

진짜 원인: MORPH (형태론)
표면 증상: LEX, SYNT 오류
```

### state/ 모듈과의 관계

```
bottleneck.ts
  └── 병목 분석 결과
          ↓
state/component-object-state.ts
  └── 구성요소별 상태 업데이트
```

---

# 제5부: 콘텐츠 생성 시스템 (Content Generation System)

이 계층은 **콘텐츠 생성**을 담당합니다. 교육적 의도 → 명세 → 생성 → 검증의 파이프라인입니다.

---

## 5.1 교육적 의도 (pedagogical-intent.ts)

> **코드 위치**: `src/core/content/pedagogical-intent.ts`
> **의존성**: `types.ts`
> **피의존**: `content-spec.ts`, `content-generator.ts`, `content-validator.ts`, `tasks/`

### 존재 이유

학습 콘텐츠는 상호 교환 가능하지 않습니다; 어휘 도입을 위한 플래시카드는 시간 제한 회상 훈련과는 다른 목적을 수행합니다. 이 모듈은 콘텐츠 구조, 난이도, 평가 기준을 주도하는 교육적 목적(의도)을 정의합니다.

### 9가지 교육적 의도

| 의도 | 설명 | 인지 부하 |
|------|------|----------|
| `introduce_new` | 새로운 어휘/개념에 대한 첫 노출 | 3 |
| `reinforce_known` | 이미 접한 자료 연습 | 2 |
| `test_comprehension` | 수용적 이해력 검증 | 3 |
| `elicit_production` | 능동적 언어 출력 요구 | 4 |
| `contextual_usage` | 자연스러운 사용 패턴 시연 | 3 |
| `error_detection` | 오류 인식 및 수정 훈련 | 4 |
| `metalinguistic` | 명시적 문법/구조 인식 | 4 |
| `fluency_building` | 속도와 자동화 구축 | 2 |
| `transfer_testing` | 알려진 패턴을 새로운 경우에 적용 | 5 |

### 파생 관계

```
pedagogical-intent.ts (기반)
        ↓
content-spec.ts (의도를 명세에 통합)
        ↓
content-generator.ts (의도에 맞는 콘텐츠 생성)
        ↓
content-validator.ts (의도 정렬 검증)
```

---

## 5.2 콘텐츠 명세 (content-spec.ts)

> **코드 위치**: `src/core/content/content-spec.ts`
> **의존성**: `pedagogical-intent.ts`, `types.ts`
> **피의존**: `content-generator.ts`, `content-validator.ts`

### 존재 이유

콘텐츠 생성에는 요청하는 컴포넌트와 생성하는 컴포넌트 간의 정밀한 계약이 필요합니다. ContentSpec은 모든 제약 조건과 요구 사항을 선행적으로 캡처합니다.

### ContentSpec 구조

```typescript
interface ContentSpec {
  // 대상
  targetObjects: LanguageObject[];

  // 교육적 의도
  intent: PedagogicalIntent;
  phase: LearningPhase;

  // 제약 조건
  difficulty: DifficultyConstraints;
  scaffolding: ScaffoldingConfig;
  context: ContentContextSpec;
  quality: ContentQualitySpec;
}
```

---

## 5.3 콘텐츠 생성기 (content-generator.ts)

> **코드 위치**: `src/core/content/content-generator.ts`
> **의존성**: `content-spec.ts`, `pedagogical-intent.ts`, `types.ts`
> **피의존**: 과제 생성 파이프라인, 캐싱 인프라

### 존재 이유

학습 애플리케이션은 학습자에게 다양하고 맥락에 적합한 콘텐츠를 제시해야 합니다. 이 모듈은 우선순위가 지정된 대체 체인을 구현합니다.

### 대체 체인 (Fallback Chain)

```
ContentSpec (무엇을 생성할지)
     |
     v
ContentGenerator.generate()
     |
     +---> tryCache() ---> [캐시 히트] ---> 캐시된 것 반환
     |
     +---> tryTemplate() ---> [템플릿 발견] ---> 적용 & 캐시 ---> 반환
     |
     +---> tryAI() ---> [API 사용 가능] ---> Claude 호출 ---> 파싱 ---> 캐시 ---> 반환
     |
     +---> generateFallback() ---> 기본 콘텐츠 반환
     |
     v
GenerationResult (콘텐츠 + 메타데이터)
```

### 설계 결정

**대체 순서 (cache → template → AI)**: 이 순서는 속도와 비용을 우선시합니다:
- 캐시 히트: 비용 없이 거의 즉각적
- 템플릿: 빠르고 결정론적
- AI: 가장 느리지만 최고 품질

---

## 5.4 콘텐츠 검증기 (content-validator.ts)

> **코드 위치**: `src/core/content/content-validator.ts`
> **의존성**: `content-spec.ts`, `pedagogical-intent.ts`, `types.ts`
> **피의존**: 콘텐츠 생성 파이프라인, 품질 게이트

### 존재 이유

생성된 콘텐츠에는 오류, 부적절한 자료, 또는 교육적으로 잘못 정렬된 작업이 포함될 수 있습니다. 이 모듈은 콘텐츠가 학습자에게 도달하기 전에 다차원 검증을 수행합니다.

### 검증 범주

| 범주 | 검사 내용 | 가중치 |
|------|----------|--------|
| linguistic | 단어 수, 문법, 어휘 수준 | 0.25 |
| pedagogical | 의도 정렬, 스캐폴딩 | 0.25 |
| technical | 구조, 명세 매칭 | 0.20 |
| safety | 적절성, PII | 0.30 |

### 품질 계층 결정

| 점수 | 계층 |
|------|------|
| 90+ | premium |
| 70+ | standard |
| <70 | fallback |

**안전 실패 시**: 다른 점수와 관계없이 fallback 계층으로 강제됩니다.

---

# 제6부: 과제 시스템 (Task System)

이 계층은 **과제 생성과 관리**를 담당합니다.

---

## 6.1 전통적 과제 유형 (traditional-task-types.ts)

> **코드 위치**: `src/core/tasks/traditional-task-types.ts`
> **의존성**: `types.ts`, `pedagogical-intent.ts`
> **피의존**: `task-constraint-solver.ts`, `distractor-generator.ts`, 과제 생성 서비스

### 존재 이유

언어 학습 연구는 서로 다른 인지 과정과 언어 구성 요소를 대상으로 하는 별개의 연습 유형을 식별했습니다. 이 모듈은 언어 교육학 문헌의 30개 전통적 과제 유형을 체계화합니다.

### 6개 카테고리, 30개 유형

| 카테고리 | 개수 | 설명 | 예시 |
|----------|------|------|------|
| **수용적** | 5 | 생산 없이 이해하기 | 독해, 청해 |
| **생산적** | 5 | 적극적인 언어 창출 | 에세이 작성, 받아쓰기 |
| **변환적** | 7 | 형태 간 변환 | 번역, 의역 |
| **빈칸 채우기** | 4 | 빈칸 완성 연습 | 클로즈 삭제, 단어 은행 |
| **상호작용적** | 4 | 대화 및 응답 | 역할극, 질문 답변 |
| **분석적** | 5 | 오류 감지 및 분석 | 오류 수정, 문법 식별 |

### TraditionalTaskTypeMeta 구조

각 과제 유형은 풍부한 메타데이터를 포함합니다:
- `masteryRange`: 이 과제가 적합한 단계(0-4)
- `cognitiveLoad`: 필요한 정신적 노력(1-5)
- `scaffoldingLevel`: 일반적으로 필요한 지원 수준(0-3)
- `needsDistractors`: MCQ 스타일 오답 선택지 필요 여부
- `requiresProduction`: 수용적 vs 생산적 구분

---

## 6.2 과제 제약 조건 솔버 (task-constraint-solver.ts)

> **코드 위치**: `src/core/tasks/task-constraint-solver.ts`
> **의존성**: `traditional-task-types.ts`, `types.ts`, `pedagogical-intent.ts`
> **피의존**: 과제 생성 서비스

### 존재 이유

과제 생성기는 각 연습에 *어떤* 어휘 항목이 나타날지 선택해야 합니다. 이 선택은 무작위일 수 없습니다: 학습자의 현재 숙달 수준, 과제의 난이도 요구 사항, 도메인 관련성, 항목의 복습 예정 여부를 고려해야 합니다.

### 점수화 기준

| 요소 | 점수 | 설명 |
|------|------|------|
| difficultyFit | 0-25 | 객체 난이도가 목표와 얼마나 잘 맞는지 |
| componentMatch | 0-20 | 객체가 목표 구성 요소를 연습하는지 |
| masteryFit | 0-25 | 학습자의 숙달 단계와의 정렬 |
| domainFit | 0-15 | 필수 도메인과의 중첩 |
| recencyScore | 0-10 | 최근/비최근 항목에 대한 선호 |
| dueScore | 0-5 | 복습 예정 항목에 대한 보너스 |

### 2단계 필터링

1. **경성 제약 조건**: 특정 사유와 함께 즉시 거부 (숙달 범위, 도메인)
2. **연성 제약 조건**: 점수에 영향 (난이도 적합성, 최근성)

---

## 6.3 오답지 생성기 (distractor-generator.ts)

> **코드 위치**: `src/core/tasks/distractor-generator.ts`
> **의존성**: `traditional-task-types.ts`, `types.ts`, `semantic-network.ts`
> **피의존**: 과제 생성 파이프라인

### 존재 이유

객관식 문제는 오답 선택지(오답지)가 믿을 만하게 틀릴 때만 효과적입니다. 무작위 오답은 학습자가 추측으로 제거할 수 있게 합니다; 잘 만들어진 오답지는 진정한 지식 인출을 강제합니다.

### 언어학적 전략

| 전략 | 설명 | 예시 |
|------|------|------|
| `phonological_similar` | 발음이 비슷한 단어 | "their" vs "there" |
| `orthographic_similar` | 철자가 비슷한 단어 | "affect" vs "effect" |
| `semantic_related` | 같은 개념 영역 | "chair" vs "table" |
| `morphological_variant` | 같은 어근의 다른 형태 | "run" vs "running" |
| `common_confusion` | 알려진 L1 간섭 패턴 | "make" vs "do" |
| `translation_false_friend` | 언어 간 의미가 다른 동족어 | "actual" (스페인어: 현재의) |

### semantic-network.ts와의 통합

```
정답: "happy"
        ↓
semantic-network.ts에서 관련 단어 조회
        ↓
오답 후보: ["joyful", "glad", "sad", "angry"]
        ↓
전략에 따라 선택: ["sad"] (반의어 전략)
```

---

# 제7부: 파생 모듈 (Derived Modules)

이 계층은 이전 계층들을 기반으로 구축된 **파생 모듈**들입니다.

---

## 7.1 문법 최적화 (grammar/)

> **디렉토리**: `src/core/grammar/`
> **의존성**: `syntactic.ts`, `content/`
> **피의존**: 과제 생성

### grammar-sequence-optimizer.ts

문법 항목의 최적 학습 순서를 결정합니다:
- 선수조건 관계
- 복잡성 점진적 증가
- 전이 가능성
- 의사소통적 유용성

### syntactic-construction.ts

언어별 문법 구문 데이터베이스로, 각 구문에 대해:
- 형태론적 패턴
- 의미적 역할
- 화용적 기능
- 난이도 등급

---

## 7.2 사용역 계산 (register/)

> **디렉토리**: `src/core/register/`
> **의존성**: `pragmatics.ts`
> **피의존**: 콘텐츠 생성, 과제 선택

### register-calculator.ts

텍스트의 사용역(격식 수준, 기술성 등)을 분석하고 계산합니다.

### register-profile.ts

특정 도메인이나 장르의 언어적 특성을 정의하는 프로필 시스템.

---

## 7.3 상태 관리 (state/)

> **디렉토리**: `src/core/state/`
> **의존성**: `types.ts`, `bottleneck.ts`
> **피의존**: 세션 관리, 분석

### component-object-state.ts

각 언어 객체에 대한 구성요소별 상태를 추적합니다:
- 음운론적 숙달
- 형태론적 숙달
- 어휘적 숙달
- 통사론적 숙달
- 화용론적 숙달

bottleneck.ts의 분석 결과가 이 상태 업데이트에 반영됩니다.

### component-search-engine.ts

상태 기반 검색으로 특정 조건을 만족하는 학습 객체를 효율적으로 찾습니다.

---

# 제8부: 수학적 기초 (Mathematical Foundations)

이 계층은 핵심 알고리즘의 **수학적 기반**을 제공합니다.

---

## 8.1 가우스-에르미트 구적법 (quadrature.ts)

> **코드 위치**: `src/core/quadrature.ts`
> **의존성**: 없음 (자체 완결형)
> **피의존**: `irt.ts` (EAP 추정 향상)

### 존재 이유

이 모듈은 LOGOS 적응형 학습 시스템에서 베이지안 능력 추정을 위한 고정밀 수치 적분을 제공합니다. 학습자의 **사후기대값(EAP)** 추정에 필요한 적분을 해석적으로 풀 수 없을 때 이를 수치적으로 계산하는 근본적인 문제를 해결합니다.

### 가우스-에르미트 vs 균일 구적법

| 방법 | 설명 | 장점 |
|------|------|------|
| 균일 구적법 | 균등 간격 점에서 샘플링 | 단순함 |
| 가우스-에르미트 | 에르미트 다항식의 근에서 샘플링 | 가우시안 사전분포에 최적 |

**쉬운 설명**: 곡선 아래 면적을 측정한다고 상상해보세요. 균등 간격의 막대를 세워 면적을 더할 수 있습니다. 하지만 수학자들은 특정 "마법의" 위치에 막대를 배치하면 훨씬 적은 막대로 더 정확한 답을 얻을 수 있다는 것을 발견했습니다.

### 정확도 vs 점 수

| 점 수 | 오차 차수 | 사용 사례 |
|-------|----------|----------|
| 5 | O(10⁻⁶) | 빠른 추정 |
| 11 | O(10⁻¹²) | 실시간 피드백 |
| 21 | O(10⁻²⁴) | 표준 세션 업데이트 |
| 41 | O(10⁻⁴⁸) | 연구용 정밀도 |

### 핵심 함수

| 함수 | 목적 |
|------|------|
| `getGaussHermiteNodes(n)` | n점에 대한 사전 계산된 노드 반환 |
| `integrateNormal(f, mean, sd, rule)` | 정규 가중치에 대해 f 적분 |
| `computeEAP(likelihood, mean, sd, rule)` | 사후 평균과 표준편차 계산 |
| `estimateThetaEAPGaussHermite(...)` | 완전한 IRT theta 추정 |

---

## 8.2 과제 매칭 (task-matching.ts)

> **코드 위치**: `src/core/task-matching.ts`
> **의존성**: `types.ts`
> **피의존**: `task-generation.service.ts`

### 존재 이유

이 모듈은 각 단어의 언어적 특성에 기반하여 어휘 단어를 최적의 연습 과제 유형에 매칭하는 정교한 알고리즘을 구현합니다. "한 가지 방식이 모두에게 맞는" 언어 학습의 문제를 해결합니다.

### z(w) 벡터 시스템

**기술적**: 단어의 언어적 특성을 나타내는 6차원 벡터.

| 차원 | 설명 | 높을 때 필요한 과제 |
|------|------|-------------------|
| F (빈도) | 코퍼스에서의 출현 빈도 | 자동화 훈련 |
| R (관계 밀도) | PMI 기반 연어 강도 | 연어 연습 |
| D (도메인 관련성) | 목표 도메인과의 관련성 | 맥락 연습 |
| M (형태론) | 형태론적 복잡성 | 단어 가족 연습 |
| P (음운론) | 발음 난이도 | 발음 훈련 |
| PRAG (화용론) | 사용역 민감도 | 적절성 연습 |

**쉬운 설명**: 각 단어에 대한 "성격 프로필"이라고 생각하세요. 사람들이 서로 다른 강점과 약점을 가진 것처럼, 단어들도 서로 다른 학습 도전을 가집니다.

### 과제 친화도 행렬

z(w) 구성요소에서 과제 유형으로의 가중 매핑:

```
collocation 과제 → relationalDensity와 0.95 친화도
pronunciation 과제 → phonological와 0.90 친화도
word_formation 과제 → morphological와 0.85 친화도
```

### 숙달 단계 제약

| 단계 | 사용 가능한 과제 |
|------|----------------|
| 0-1 | recognition, recall_cued |
| 2 | + collocation, word_formation |
| 3 | + context_sentence, register_shift |
| 4 | + rapid_response (모든 과제) |

---

## 8.3 단계 임계값 (stage-thresholds.ts)

> **코드 위치**: `src/core/stage-thresholds.ts`
> **의존성**: `types.ts`
> **피의존**: `scoring-update.service.ts`

### 존재 이유

이 모듈은 학습자가 **숙달 단계를 통해 진행하는 기준**을 정의하고 관리합니다. 누군가가 다음 단계로 진행할 만큼 충분히 "배웠다"고 판단하는 시점을 결정하는 적응형 학습의 핵심 과제를 해결합니다.

### 5단계 숙달 모델

| 단계 | 이름 | 의미 |
|------|------|------|
| 0 | 신규/미지 | 이 항목을 접한 적 없음 |
| 1 | 인식 | 힌트/단서로 식별 가능 |
| 2 | 회상 | 노력으로 기억 가능 |
| 3 | 통제된 산출 | 집중하여 산출 가능 |
| 4 | 자동화 | 유창하고 빠르며 단서 없는 접근 |

### 단계 전환 기준

| 전환 | 필요 조건 |
|------|----------|
| 0→1 | 50% cue-assisted 정확도 |
| 1→2 | 60% cue-free 또는 80% cue-assisted |
| 2→3 | 75% cue-free + 7일 안정성 |
| 3→4 | 90% cue-free + 30일 안정성 + <10% 스캐폴딩 갭 |

### 스캐폴딩 갭

**기술적**: cue-assisted 정확도와 cue-free 정확도 간의 차이.

**쉬운 설명**: 보조바퀴를 달고 자전거를 탈 때와 달지 않고 탈 때의 차이입니다. 진정한 숙달(4단계)은 높은 cue-free 정확도와 작은 갭을 모두 요구하여, 보조바퀴에 의존하지 않음을 증명합니다.

### 사전 정의된 설정

| 설정 | 4단계 정확도 | 4단계 안정성 | 사용 사례 |
|------|-------------|-------------|----------|
| 기본 | 90% | 30일 | 표준 학습 |
| 보수적 | 95% | 45일 | 전문 용어 학습 |
| 적극적 | 85% | 21일 | 빠른 학습자 |
| 연구용 | 95% | 60일 | 학술 연구 |

---

# 제9부: 서비스 계층 (Service Layer)

이 계층은 core 알고리즘을 **애플리케이션 로직**으로 조합합니다.

---

## 9.1 에이전트 트리거 서비스 (agent-trigger.ts)

> **코드 위치**: `src/main/services/agent-trigger.ts`
> **의존성**: 없음 (자체 완결형)
> **피의존**: 주 오케스트레이션 계층

### 존재 이유

이 모듈은 다중 에이전트 시스템의 근본적인 조정 문제를 해결합니다: **어떤 전문가를 언제 호출할지 어떻게 알 수 있는가?**

### 아키텍처적 위치

```
계층 0: 명세 문서 (FINAL-SPEC.md, AGENT-MANIFEST.md)
    |
계층 1: 이 모듈 - 에이전트 조정/라우팅 결정  ◄── 현재 위치
    |
계층 2: 개별 에이전트 (frontend-specialist, database-specialist 등)
    |
계층 3: 애플리케이션 코드 (src/main, src/renderer, src/core)
```

### 트리거 컨텍스트

**기술적**: 수행 중인 작업, 관련 파일, 영향받는 아키텍처 계층, 특수 조건을 캡처하는 인터페이스.

**쉬운 설명**: 현재 작업에 대한 "작업 설명서"입니다. 배관 작업을 "욕실 리모델링, 싱크대 아래 파이프 관련, 허가 필요"로 설명하듯이, 트리거 컨텍스트는 코딩 작업을 "로그인 흐름 수정, auth/login.ts 관련, IPC 계층 영향, 보안 민감"으로 설명합니다.

### 계층-에이전트 매핑

| 계층 | 전문 에이전트 |
|------|-------------|
| ui | frontend-specialist |
| ipc | api-specialist |
| db | database-specialist |
| core | general-purpose |
| service | api-specialist |

---

## 9.2 과제 생성 서비스 (task-generation.service.ts)

> **코드 위치**: `src/main/services/task-generation.service.ts`
> **의존성**: `mastery.repository`, `collocation.repository`, `state-priority.service`
> **피의존**: 학습 세션 컨트롤러

### 존재 이유

이 서비스는 LOGOS 학습 파이프라인의 **Phase 3.2: Layer 2**를 구현합니다. Layer 1이 *무엇을* 가르칠지 결정하는 동안, Layer 2는 *어떻게* 가르칠지 결정합니다.

### 데이터 흐름

```
LearningQueueItem (Layer 1에서)
        |
        v
generateTaskSpec() - 형식, 단서 수준, 난이도 결정
        |
        v
generateTask() - 프롬프트, 옵션, 힌트가 포함된 전체 과제 생성
        |
        v
cacheTask() - 미래 검색을 위해 저장
        |
        v
GeneratedTask (UI 계층으로)
```

### 과제 형식 진행

| 단계 | 과제 형식 | 인지 요구 |
|------|----------|----------|
| 0 | MCQ | 인식 |
| 1 | fill_blank | 단서 있는 회상 |
| 2 | matching | 연결 |
| 3 | ordering | 순서화 |
| 4 | free_response | 산출 |

### 단서 수준 시스템 (Gap 2.4 알고리즘)

| 수준 | 이름 | 학습자가 보는 것 |
|------|------|----------------|
| 0 | 없음 | 힌트 없음 - 완전한 회상 필요 |
| 1 | 최소 | 첫 글자 힌트: "'A'로 시작" |
| 2 | 중간 | 길이 + 부분 공개: "7글자, 'App...'로 시작" |
| 3 | 완전 | 강한 스캐폴딩: "App____" (단어의 절반 표시) |

---

## 9.3 채점 + 업데이트 서비스 (scoring-update.service.ts)

> **코드 위치**: `src/main/services/scoring-update.service.ts`
> **의존성**: `mastery.repository`, `session.repository`, `error-analysis.repository`, `goal.repository`
> **피의존**: 학습 세션 컨트롤러, 세션 종료 핸들러

### 존재 이유

이 서비스는 핵심 질문에 답함으로써 적응형 학습 루프를 완성합니다: *"학습자가 어떻게 했고, 그 결과 무엇이 바뀌어야 하는가?"*

### 아키텍처적 위치 (Layer 3)

```
사용자가 과제 봄 <---- Layer 2: 과제 생성
        |
        |  사용자가 응답 제출
        v
=====> Layer 3: 채점 + 업데이트 (이 서비스) <=====
        |        - 정확성 평가
        |        - 숙달 상태 업데이트
        |        - FSRS 스케줄링 적용
        |        - theta (능력) 업데이트
        |        - 우선순위 재계산
        |        - 오류 분석
        v
숙달 데이터 업데이트됨 -----> Layer 1로 피드
```

### 12단계 처리 파이프라인

| 단계 | 작업 | 업데이트되는 데이터 |
|------|------|-------------------|
| 1 | `evaluateResponse()` | 평가 결과 계산 |
| 2 | LanguageObject + MasteryState 조회 | 현재 숙달 데이터 검색 |
| 3 | 효과적인 단서 수준 계산 | 사용된 힌트에 따라 조정 |
| 4 | `recordResponse()` | 세션에 Response 레코드 생성 |
| 5 | `recordExposure()` | 노출 횟수 + 정확도 EMA 업데이트 |
| 6 | 업데이트된 MasteryState 조회 | EMA 후 새 정확도 획득 |
| 7 | `determineStageTransition()` | 승급/강등 확인 |
| 8 | FSRS 업데이트 | 간격 반복 스케줄링 업데이트 |
| 9 | 우선순위 업데이트 | 우선순위 재계산 및 저장 |
| 10 | `recordTaskType()` | 유창성 vs 다양성 균형 추적 |
| 11 | theta 업데이트 | 사용자 능력 업데이트 (학습 모드가 아닌 경우) |
| 12 | 오류 분석 | 오류 분류 (오답인 경우) |

### 부분 점수

**기술적**: Levenshtein 거리 기반 유사도를 사용하여 부분 점수 할당.

**쉬운 설명**: 관대한 철자 선생님처럼. "receive" 대신 "recieve"를 쓰면, 시스템은 당신이 단어를 알지만 작은 실수를 했다는 것을 인식합니다. 완전히 틀렸다고 표시하는 대신, 지식을 인정하면서 연습을 위해 오류를 표시하는 부분 점수를 줍니다.

---

# 부록: 학술적 배경

LOGOS는 다음 언어 습득 이론에 기반합니다:

## 처리가능성 이론 (Pienemann, 1998)

구성요소 선수조건 체인: PHON → MORPH → LEX → SYNT → PRAG

학습자는 선수조건이 충족된 후에만 상위 수준 처리가 가능합니다. 이것이 병목 탐지의 이론적 기반입니다.

## 전이 학습 (Perkins & Salomon, 1992)

맥락 간 일반화 추정:
- "동일 요소" 전이 vs "원리 기반" 전이

이것이 transfer.ts의 이론적 기반입니다.

## FSRS-4 (Free Spaced Repetition Scheduler)

망각 곡선 기반 복습 스케줄링:
- 난이도와 안정성 추적
- 개인화된 복습 간격

## IRT (문항반응이론)

- 1PL/2PL/3PL 모델 지원
- 개인별 능력(theta) 추정
- 적응형 테스트

## FRE 우선순위 공식

- **F**: Frequency (빈도)
- **R**: Relational density (관계 밀도)
- **E**: Domain relevance (도메인 관련성)

## 주요 참고문헌

- Miller, G.A. (1995). WordNet: A Lexical Database for English
- Bock & Mislevy (1982). EAP estimation with Gaussian quadrature
- Chang & Ying (1996). KL divergence item selection
- Yap & Balota (2015). Visual word recognition
- Segalowitz & Hulstijn (2005). Automaticity in L2 learning
- Pienemann, M. (1998). Language Processing and Second Language Development

---

> **이 문서는 LOGOS 코드베이스의 모든 섀도우 문서를 인과관계적 파생 위계구조로 통합한 것입니다.**
> **문서 생성일**: 2026-01-07
> **생성자**: Claude Opus 4.5
